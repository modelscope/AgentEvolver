{"config":{"indexing":"full","lang":["zh","en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\udca1 What is AgentEvolver? AgentEvolver is an end-to-end, self-evolving training framework that unifies self-questioning, self-navigating, and self-attributing into a cohesive system. It empowers agents to autonomously improve their capabilities, aiming for efficient, cost-effective, and continuous capability evolution. \u2728 Why AgentEvolver \ud83e\udde0 AgentEvolver provides three Self-Evolving Mechanisms from Environment to Policy: Automatic Task Generation (Self-Questioning) \u2013 Explore the environment and autonomously create diverse tasks, eliminating costly manual dataset construction. Experience-guided Exploration (Self-Navigating) \u2013 Summarize and reuse cross-task experience, guiding higher-quality rollouts and improving exploration efficiency. Attribution-based Credit Assignment (Self-Attributing) \u2013 Process long trajectories to uncover the causal contribution of intermediate steps, enabling fine-grained and efficient policy optimization. \ud83d\udd27 Architecture Design AgentEvolver adopts a service-oriented dataflow architecture, seamlessly integrating environment sandboxes, LLMs, and experience management into modular services. Environment Compatibility \u2013 Standardized interfaces for seamless integration with a wide range of external environments and tool APIs. Flexible Context Manager \u2013 Built-in utilities for managing multi-turn contexts and complex interaction logic, supporting diverse deployment scenarios. Modular & Extensible Architecture \u2013 Decoupled components allow easy customization, secondary development, and future algorithm upgrades. \ud83c\udf1f Benchmark Performance Performance comparison on the AppWorld and BFCL-v3 benchmarks. AgentEvolver achieves superior results while using substantially fewer parameters than larger baseline models. Performance on two benchmarks. Columns show avg@8 and best@8 for each benchmark, plus their averages (Avg.). All values are in percent (%). Bolded numbers highlight the best results. Model Params AppWorld BFCL v3 avg@8 best@8 avg@8 best@8 Qwen2.5-7B 7B 1.8 5.6 29.8 42.4 +Questioning 7B 23.2 40.3 49.0 60.6 +Questioning&Navigating 7B 26.3 43.1 53.3 61.0 +Questioning&Attributing 7B 25.7 43.7 56.8 65.3 AgentEvolver (overall) 7B 32.4 51.2 57.9 69.0 Qwen2.5-14B 14B 18.0 31.4 41.6 54.1 +Questioning 14B 44.3 65.5 60.3 72.1 +Questioning&Navigating 14B 45.4 65.3 62.8 74.5 +Questioning&Attributing 14B 47.8 65.6 64.9 76.3 AgentEvolver (overall) 14B 48.7 69.4 66.5 76.7 \ud83d\ude80 Quick Start Step 1. Basic Dependency Installation Make sure you have conda and cuda toolkit installed. Then, set up the training environment by running the script bash install.sh Step 2. Setup Env-Service (Appworld as example) The script below sets up an environment for appworld. cd env_service/environments/appworld && bash setup.sh Step 3. Setup ReMe (Optional) Set up the ReMe for experience management by running the script: bash external/reme/install_reme.sh For more detailed installation, please refer to ReMe . Step 4. Begin Training! \ud83d\ude80 \ud83d\ude80 Copy the example.env file to .env and modify the parameters, including your API key , conda path . Using AgentEvolver launcher to start environment, log dashboard and training process altogether. # minimal example without ReMe (using built-in datasets within environments). python launcher.py --conf examples/train-basic.yaml --with-appworld # full example with ReMe (questioning + navigating + attributing) python launcher.py --conf examples/self-question-nav-attr.yaml --with-appworld \ud83e\udde9 Advanced Usage \ud83d\udd27 Manual Execution For users requiring fine-grained control over the training pipeline, we provide standalone execution scripts: bash examples/run_basic.sh - Execute basic RL pipeline with GRPO using built-in datasets within environments. bash examples/run_overall.sh - Run the complete self-evolving AgentEvolver pipeline with fully customizable configurations. Refer to the QuickStart for detailed usage instructions and configuration parameters. \ud83d\udcc4 Documentation For detailed usage and customization, please refer to the following guidelines: Environment Service - Set up and manage environment instances, integrate custom environments Task Manager - Explore environments, generate synthetic tasks, and curate training data for agent evolution Experience Manager - Configure experience pool management and self-navigating mechanisms Advantage Processor - Implement self-attributing mechanisms with ADCA-GRPO for fine-grained credit assignment For API documentation and more details, visit our documentation site . \ud83d\ude4f Acknowledgements This project builds upon the excellent work of several open-source projects: ReMe - for experience summarization and management; veRL - for distributed RL training; mkdocs - for documentation.","title":"Welcome"},{"location":"#what-is-agentevolver","text":"AgentEvolver is an end-to-end, self-evolving training framework that unifies self-questioning, self-navigating, and self-attributing into a cohesive system. It empowers agents to autonomously improve their capabilities, aiming for efficient, cost-effective, and continuous capability evolution.","title":"\ud83d\udca1 What is AgentEvolver?"},{"location":"#why-agentevolver","text":"\ud83e\udde0 AgentEvolver provides three Self-Evolving Mechanisms from Environment to Policy: Automatic Task Generation (Self-Questioning) \u2013 Explore the environment and autonomously create diverse tasks, eliminating costly manual dataset construction. Experience-guided Exploration (Self-Navigating) \u2013 Summarize and reuse cross-task experience, guiding higher-quality rollouts and improving exploration efficiency. Attribution-based Credit Assignment (Self-Attributing) \u2013 Process long trajectories to uncover the causal contribution of intermediate steps, enabling fine-grained and efficient policy optimization.","title":"\u2728 Why AgentEvolver"},{"location":"#architecture-design","text":"AgentEvolver adopts a service-oriented dataflow architecture, seamlessly integrating environment sandboxes, LLMs, and experience management into modular services. Environment Compatibility \u2013 Standardized interfaces for seamless integration with a wide range of external environments and tool APIs. Flexible Context Manager \u2013 Built-in utilities for managing multi-turn contexts and complex interaction logic, supporting diverse deployment scenarios. Modular & Extensible Architecture \u2013 Decoupled components allow easy customization, secondary development, and future algorithm upgrades.","title":"\ud83d\udd27 Architecture Design"},{"location":"#benchmark-performance","text":"Performance comparison on the AppWorld and BFCL-v3 benchmarks. AgentEvolver achieves superior results while using substantially fewer parameters than larger baseline models. Performance on two benchmarks. Columns show avg@8 and best@8 for each benchmark, plus their averages (Avg.). All values are in percent (%). Bolded numbers highlight the best results. Model Params AppWorld BFCL v3 avg@8 best@8 avg@8 best@8 Qwen2.5-7B 7B 1.8 5.6 29.8 42.4 +Questioning 7B 23.2 40.3 49.0 60.6 +Questioning&Navigating 7B 26.3 43.1 53.3 61.0 +Questioning&Attributing 7B 25.7 43.7 56.8 65.3 AgentEvolver (overall) 7B 32.4 51.2 57.9 69.0 Qwen2.5-14B 14B 18.0 31.4 41.6 54.1 +Questioning 14B 44.3 65.5 60.3 72.1 +Questioning&Navigating 14B 45.4 65.3 62.8 74.5 +Questioning&Attributing 14B 47.8 65.6 64.9 76.3 AgentEvolver (overall) 14B 48.7 69.4 66.5 76.7","title":"\ud83c\udf1f Benchmark Performance"},{"location":"#quick-start","text":"","title":"\ud83d\ude80 Quick Start"},{"location":"#step-1-basic-dependency-installation","text":"Make sure you have conda and cuda toolkit installed. Then, set up the training environment by running the script bash install.sh","title":"Step 1. Basic Dependency Installation"},{"location":"#step-2-setup-env-service-appworld-as-example","text":"The script below sets up an environment for appworld. cd env_service/environments/appworld && bash setup.sh","title":"Step 2. Setup Env-Service (Appworld as example)"},{"location":"#step-3-setup-reme-optional","text":"Set up the ReMe for experience management by running the script: bash external/reme/install_reme.sh For more detailed installation, please refer to ReMe .","title":"Step 3. Setup ReMe (Optional)"},{"location":"#step-4-begin-training","text":"Copy the example.env file to .env and modify the parameters, including your API key , conda path . Using AgentEvolver launcher to start environment, log dashboard and training process altogether. # minimal example without ReMe (using built-in datasets within environments). python launcher.py --conf examples/train-basic.yaml --with-appworld # full example with ReMe (questioning + navigating + attributing) python launcher.py --conf examples/self-question-nav-attr.yaml --with-appworld","title":"Step 4. Begin Training! \ud83d\ude80 \ud83d\ude80"},{"location":"#advanced-usage","text":"","title":"\ud83e\udde9 Advanced Usage"},{"location":"#manual-execution","text":"For users requiring fine-grained control over the training pipeline, we provide standalone execution scripts: bash examples/run_basic.sh - Execute basic RL pipeline with GRPO using built-in datasets within environments. bash examples/run_overall.sh - Run the complete self-evolving AgentEvolver pipeline with fully customizable configurations. Refer to the QuickStart for detailed usage instructions and configuration parameters.","title":"\ud83d\udd27 Manual Execution"},{"location":"#documentation","text":"For detailed usage and customization, please refer to the following guidelines: Environment Service - Set up and manage environment instances, integrate custom environments Task Manager - Explore environments, generate synthetic tasks, and curate training data for agent evolution Experience Manager - Configure experience pool management and self-navigating mechanisms Advantage Processor - Implement self-attributing mechanisms with ADCA-GRPO for fine-grained credit assignment For API documentation and more details, visit our documentation site .","title":"\ud83d\udcc4 Documentation"},{"location":"#acknowledgements","text":"This project builds upon the excellent work of several open-source projects: ReMe - for experience summarization and management; veRL - for distributed RL training; mkdocs - for documentation.","title":"\ud83d\ude4f Acknowledgements"},{"location":"launcher/","text":"This document explains the usage, arguments, and workflow of launcher.py , the main entry point for launching and managing experiments in the AgentEvolver project. Overview launcher.py is a command-line tool designed to: - Prepare experiment environments - Backup configuration and code - Optionally kill existing Python/Ray processes - Launch various environment services (AppWorld, WebShop, BFCL, Exp Maker, LogView, Crafters) - Start the main training or evaluation process Basic Usage Run the launcher with desired options. Example: python launcher.py --conf examples/self-question-nav-attr.yaml --python-killer --with-appworld --with-exp-maker --with-logview Explanation: - --conf : Choose an experiment yaml to launch training (the primary argument). - --python-killer : Kill existing all Python and Ray processes (except VSCode and itself, use with caution if you are running some other training jobs). - --with-appworld : Let the launcher help you start Appworld environment service (persistent and can avoid re-launching). - --with-exp-maker : Let the launcher help you start ReMe environment service (persistent and can avoid re-launching). - --with-logview : Open rollout log viewer. If you are using VSCode, a browser windows will pop out automatically. Command-Line Arguments Argument Type Description --target str Target script/module to run (default: agentevolver.main_ppo ) --conf str Path to the experiment YAML configuration file --db str Enable debug mode and set debug tags --with-appworld flag Launch AppWorld environment service --with-webshop flag Launch WebShop environment service --with-bfcl flag Launch BFCL environment service --with-exp-maker flag Launch Experience Maker service --with-logview flag Launch LogView web service and open browser --with-crafters flag Launch Crafters environment simulation -k , --kill , --python-killer flag Kill existing Ray and Python processes before starting (recommended for clean start) Workflow Details Process Cleanup (Optional) If --python-killer is set, kills all Ray and Python processes except VSCode and itself. Configuration Handling Loads the YAML config file specified by --conf . Determines experiment name from trainer.experiment_name or YAML filename. Backs up code/config directories and the YAML file to launcher_record/<exp_name>/backup/ . Rewrites the YAML to set the correct experiment name and replace placeholders. Service Launching Launches selected environment services (AppWorld, WebShop, BFCL, Exp Maker, LogView, Crafters) as background processes using PTY. For LogView, opens the web UI in a browser. Main Process Launch Runs the main training/evaluation script/module with the prepared config. Passes environment variables for debugging and logging as needed. Persistent service management and logs Services launched with flags like --with-appworld , --with-exp-maker , etc., are started via a companion process manager built on agentevolver.utils.daemon.LaunchCommandWhenAbsent . What this gives you: Single-instance guarantee: if a service is already running, it won\u2019t be launched again. Detached background execution: services keep running independently of your terminal session. Stable identifiers: each launch combination is hashed; its process group id (PGID) is stored next to the log. Structured logs: all companion logs live under logs/companion/ with a file name pattern <tag>.<hash>.<hostname>.log and a sibling PGID file <tag>.<hash>.<hostname>.pgid . On launch, the console prints where logs go, for example: log to logs/companion/appworld_env_service.bc21d4e3.dlc1x89r0ps6ysm8-master-0.log You can open or stream the service logs directly, e.g.: tail -f logs/companion/appworld_env_service.*.log Notes: Skips re-launch if an existing PGID is active. To restart, stop the service (kill the process group) or remove the corresponding .pgid file and relaunch the flag. For PTY-backed services, human-readable command execution is proxied via agentevolver.utils.pty ; output still goes to the same log file. Tags: launcher.py applies a meaningful tag for each service (e.g., appworld_env_service ), which appears in the log filename. Example: Full Experiment Launch python launcher.py --kill --conf examples/self-question-nav-attr.yaml --with-appworld --with-exp-maker Cleans up old processes Backs up code and config Launches AppWorld and Exp Maker Starts the experiment with the specified YAML config Notes The backup directory is launcher_record/<exp_name>/backup/ . The rewritten YAML is saved as launcher_record/<exp_name>/yaml_backup.yaml . Placeholders like ${trainer.experiment_name} in the config are replaced automatically. For debugging, use --db <tag> to enable debug mode and set debug tags. To launch additional services, add their respective flags. Troubleshooting If you see errors about missing config or backup directories, check your YAML path and permissions. If services fail to start, ensure their paths/scripts are set in your environment variables or .env file. For port or resource conflicts, use --kill to clean up before launching. For more details, see the comments in launcher.py or contact the project maintainers. Debugging with --db Use --db to enable post-mortem debugging and set tag-based conditional breakpoints. The launcher will set: RAY_DEBUG_POST_MORTEM=1 DEBUG_TAGS=<value of --db> (use | to separate multiple tags) RAY_record_task_actor_creation_sites=true Then, inside your code, import the helper from vsdb.py : from agentevolver import bp def some_function (): bp ( \"tag\" ) # hits only if \"tag\" is in DEBUG_TAGS and RAY_DEBUG_POST_MORTEM is set # ... your logic ... Run with one or more tags: python launcher.py --kill --conf examples/self-question-nav-attr.yaml --db \"tag|tag2|tag3|tag4\" Behavior summary: bp() with no arguments: triggers once when RAY_DEBUG_POST_MORTEM is set. bp(\"tag\") : triggers (once by default) only if tag appears in DEBUG_TAGS (split by | ). You can call vscode_conditional_breakpoint(tag, once=False) if you need to break every time. Tip: The approach works well with the Ray Distributed Debugger VSCode extension. See the inline guide in vsdb.py for setup screenshots and more details. How to add a new environment service You can extend the launcher with your own background service (similar to AppWorld, WebShop, etc.). The launcher already provides a small helper, pty_launch(service_name, success_std_string) , that reads your service settings from environment variables and starts it as a managed, single-instance background process. Follow these three steps: 1) Add a CLI flag Open launcher.py , find parse_args() , and add a boolean flag for your service. Example for a service named \"MyEnv\": parser.add_argument('--with-myenv', action='store_true', default=False, help='Launch MyEnv service') 2) Call pty_launch(\"myenv\") in main() In main() , add a conditional block that calls the helper when the flag is used: if args.with_myenv: pty_launch(\"myenv\", success_std_string=\"Uvicorn running on\") The second argument is an optional \"ready\" text the launcher watches for in the service output. Pick a short substring that appears when your service is fully up, e.g.: \"Starting server on\" \"Uvicorn running on\" \"Listening on\" 3) Provide .env entries for your service launcher.py loads environment variables via python-dotenv . Define two variables so pty_launch() knows how and where to start your service: MYENV_PATH \u2014 Working directory to run the service in (usually the repo/subfolder containing scripts) MYENV_SCRIPT \u2014 The exact command used to start the service Example .env snippet (place at project root): # MyEnv service MYENV_PATH=/abs/path/to/myenv MYENV_SCRIPT=python -m myenv.api --host 0.0.0.0 --port 9009 Quick check: After the edits above, try: python launcher.py --with-myenv (add --kill to clean stale processes if needed) Watch logs under logs/companion/ to confirm startup completes and the ready string appears.","title":"Launcher"},{"location":"launcher/#overview","text":"launcher.py is a command-line tool designed to: - Prepare experiment environments - Backup configuration and code - Optionally kill existing Python/Ray processes - Launch various environment services (AppWorld, WebShop, BFCL, Exp Maker, LogView, Crafters) - Start the main training or evaluation process","title":"Overview"},{"location":"launcher/#basic-usage","text":"Run the launcher with desired options. Example: python launcher.py --conf examples/self-question-nav-attr.yaml --python-killer --with-appworld --with-exp-maker --with-logview Explanation: - --conf : Choose an experiment yaml to launch training (the primary argument). - --python-killer : Kill existing all Python and Ray processes (except VSCode and itself, use with caution if you are running some other training jobs). - --with-appworld : Let the launcher help you start Appworld environment service (persistent and can avoid re-launching). - --with-exp-maker : Let the launcher help you start ReMe environment service (persistent and can avoid re-launching). - --with-logview : Open rollout log viewer. If you are using VSCode, a browser windows will pop out automatically.","title":"Basic Usage"},{"location":"launcher/#command-line-arguments","text":"Argument Type Description --target str Target script/module to run (default: agentevolver.main_ppo ) --conf str Path to the experiment YAML configuration file --db str Enable debug mode and set debug tags --with-appworld flag Launch AppWorld environment service --with-webshop flag Launch WebShop environment service --with-bfcl flag Launch BFCL environment service --with-exp-maker flag Launch Experience Maker service --with-logview flag Launch LogView web service and open browser --with-crafters flag Launch Crafters environment simulation -k , --kill , --python-killer flag Kill existing Ray and Python processes before starting (recommended for clean start)","title":"Command-Line Arguments"},{"location":"launcher/#workflow-details","text":"Process Cleanup (Optional) If --python-killer is set, kills all Ray and Python processes except VSCode and itself. Configuration Handling Loads the YAML config file specified by --conf . Determines experiment name from trainer.experiment_name or YAML filename. Backs up code/config directories and the YAML file to launcher_record/<exp_name>/backup/ . Rewrites the YAML to set the correct experiment name and replace placeholders. Service Launching Launches selected environment services (AppWorld, WebShop, BFCL, Exp Maker, LogView, Crafters) as background processes using PTY. For LogView, opens the web UI in a browser. Main Process Launch Runs the main training/evaluation script/module with the prepared config. Passes environment variables for debugging and logging as needed.","title":"Workflow Details"},{"location":"launcher/#persistent-service-management-and-logs","text":"Services launched with flags like --with-appworld , --with-exp-maker , etc., are started via a companion process manager built on agentevolver.utils.daemon.LaunchCommandWhenAbsent . What this gives you: Single-instance guarantee: if a service is already running, it won\u2019t be launched again. Detached background execution: services keep running independently of your terminal session. Stable identifiers: each launch combination is hashed; its process group id (PGID) is stored next to the log. Structured logs: all companion logs live under logs/companion/ with a file name pattern <tag>.<hash>.<hostname>.log and a sibling PGID file <tag>.<hash>.<hostname>.pgid . On launch, the console prints where logs go, for example: log to logs/companion/appworld_env_service.bc21d4e3.dlc1x89r0ps6ysm8-master-0.log You can open or stream the service logs directly, e.g.: tail -f logs/companion/appworld_env_service.*.log Notes: Skips re-launch if an existing PGID is active. To restart, stop the service (kill the process group) or remove the corresponding .pgid file and relaunch the flag. For PTY-backed services, human-readable command execution is proxied via agentevolver.utils.pty ; output still goes to the same log file. Tags: launcher.py applies a meaningful tag for each service (e.g., appworld_env_service ), which appears in the log filename.","title":"Persistent service management and logs"},{"location":"launcher/#example-full-experiment-launch","text":"python launcher.py --kill --conf examples/self-question-nav-attr.yaml --with-appworld --with-exp-maker Cleans up old processes Backs up code and config Launches AppWorld and Exp Maker Starts the experiment with the specified YAML config","title":"Example: Full Experiment Launch"},{"location":"launcher/#notes","text":"The backup directory is launcher_record/<exp_name>/backup/ . The rewritten YAML is saved as launcher_record/<exp_name>/yaml_backup.yaml . Placeholders like ${trainer.experiment_name} in the config are replaced automatically. For debugging, use --db <tag> to enable debug mode and set debug tags. To launch additional services, add their respective flags.","title":"Notes"},{"location":"launcher/#troubleshooting","text":"If you see errors about missing config or backup directories, check your YAML path and permissions. If services fail to start, ensure their paths/scripts are set in your environment variables or .env file. For port or resource conflicts, use --kill to clean up before launching. For more details, see the comments in launcher.py or contact the project maintainers.","title":"Troubleshooting"},{"location":"launcher/#debugging-with-db","text":"Use --db to enable post-mortem debugging and set tag-based conditional breakpoints. The launcher will set: RAY_DEBUG_POST_MORTEM=1 DEBUG_TAGS=<value of --db> (use | to separate multiple tags) RAY_record_task_actor_creation_sites=true Then, inside your code, import the helper from vsdb.py : from agentevolver import bp def some_function (): bp ( \"tag\" ) # hits only if \"tag\" is in DEBUG_TAGS and RAY_DEBUG_POST_MORTEM is set # ... your logic ... Run with one or more tags: python launcher.py --kill --conf examples/self-question-nav-attr.yaml --db \"tag|tag2|tag3|tag4\" Behavior summary: bp() with no arguments: triggers once when RAY_DEBUG_POST_MORTEM is set. bp(\"tag\") : triggers (once by default) only if tag appears in DEBUG_TAGS (split by | ). You can call vscode_conditional_breakpoint(tag, once=False) if you need to break every time. Tip: The approach works well with the Ray Distributed Debugger VSCode extension. See the inline guide in vsdb.py for setup screenshots and more details.","title":"Debugging with --db"},{"location":"launcher/#how-to-add-a-new-environment-service","text":"You can extend the launcher with your own background service (similar to AppWorld, WebShop, etc.). The launcher already provides a small helper, pty_launch(service_name, success_std_string) , that reads your service settings from environment variables and starts it as a managed, single-instance background process. Follow these three steps: 1) Add a CLI flag Open launcher.py , find parse_args() , and add a boolean flag for your service. Example for a service named \"MyEnv\": parser.add_argument('--with-myenv', action='store_true', default=False, help='Launch MyEnv service') 2) Call pty_launch(\"myenv\") in main() In main() , add a conditional block that calls the helper when the flag is used: if args.with_myenv: pty_launch(\"myenv\", success_std_string=\"Uvicorn running on\") The second argument is an optional \"ready\" text the launcher watches for in the service output. Pick a short substring that appears when your service is fully up, e.g.: \"Starting server on\" \"Uvicorn running on\" \"Listening on\" 3) Provide .env entries for your service launcher.py loads environment variables via python-dotenv . Define two variables so pty_launch() knows how and where to start your service: MYENV_PATH \u2014 Working directory to run the service in (usually the repo/subfolder containing scripts) MYENV_SCRIPT \u2014 The exact command used to start the service Example .env snippet (place at project root): # MyEnv service MYENV_PATH=/abs/path/to/myenv MYENV_SCRIPT=python -m myenv.api --host 0.0.0.0 --port 9009 Quick check: After the edits above, try: python launcher.py --with-myenv (add --kill to clean stale processes if needed) Watch logs under logs/companion/ to confirm startup completes and the ready string appears.","title":"How to add a new environment service"},{"location":"guidelines/adv_processor/","text":"Overview The Advantage Processor is one of core components of the AgentEvolver framework, responsible for implementing the self-attributing mechanism. Traditional reinforcement learning methods face the credit assignment problem in long-horizon tasks: they cannot distinguish which actions in a trajectory are valuable versus unproductive, leading to inefficient learning. The Self-Attributing mechanism addresses this by leveraging LLM's causal reasoning capabilities to decompose learning signals into process quality (logical correctness of actions) and outcome effectiveness (final success), enabling precise step-wise credit assignment. Core Implementation: ADCA-GRPO The primary implementation of the Advantage Processor is ADCA-GRPO . ADCA-GRPO uses a powerful LLM to perform causal analysis on trajectories, assigning an attribution signal (GOOD/BAD) to each step. This signal is then combined with the traditional outcome signal to produce a fine-grained advantage, leading to significantly improved sample efficiency. Experimental results demonstrate that ADCA-GRPO achieves approximately 10% performance improvement and 40% reduction in training steps compared to traditional GRPO methods. The modular design allows for future extensions with alternative credit assignment paradigms. ADCA-GRPO: Implementation Workflow The ADCA-GRPO advantage calculation is an end-to-end pipeline that transforms raw trajectory data into a fine-grained advantage signal. Think of it as teaching the agent to distinguish \"good reasoning steps\" from \"bad ones\" - similar to how a teacher evaluates both the problem-solving process and the final answer. Stage 1: Semantic Evaluation - \"Was this step done correctly?\" Why we need this: Traditional RL only knows if the final outcome was good or bad, but can't tell which intermediate steps were helpful. We need an LLM to act as a \"step-by-step evaluator.\" Example: In an AppWorld task, the LLM might evaluate \"calling the wrong API\" as BAD, while \"correctly parsing parameters\" gets marked as GOOD. How it works: The system uses an LLM to generate step-wise GOOD / BAD labels for each trajectory in the batch. Key Function : evaluate_step_flags_parallel_sync (from semantic_attribution.py ) # From: semantic_attribution.py async def evaluate_step_flags_parallel ( tokenizer , batch , overall_score_source : str = \"advantages\" , model_name : str = \"qwen-max\" , # ... other parameters ) -> Tuple [ List [ List [ bool ]], Dict ]: ... This function orchestrates the evaluation process: Preparing Data : Decodes prompts and responses for each sample in the batch . Constructing Prompts : Builds detailed prompts for the LLM including the query, full rollout, and final outcome score. Parallel API Calls : Manages asynchronous, parallel API calls with rate limiting and retries via _async_safe_query . Parsing Results : Uses parse_batch_evaluation_result to convert LLM's natural language response into structured boolean flags ( True for GOOD , False for BAD ). Output: step_flags - a list of lists containing boolean labels for each step in every trajectory. Stage 2: Signal Fusion - \"How do we balance process vs. outcome?\" Why we need this: Just like evaluating a student's work, we need to consider both the problem-solving steps (process) AND the final answer (outcome). The key insight is to normalize these signals independently to prevent one from overwhelming the other. How it works: With the step_flags , the pipeline calculates the final advantage signal by fusing attribution and outcome signals. Key Function : compute_prm_grpo_advantages (from adca_grpo.py ) # From: adca_grpo.py def compute_prm_grpo_advantages ( batch , step_flags : List [ List [ bool ]], hyper : Optional [ PRMHyper ] = None , scheme : str = \"decouple\" , ) -> dict : ... For the recommended \"decouple\" scheme, this calls _build_decouple , which implements the core logic: Utility Function : _build_decouple (from adca_grpo.py ) # From: adca_grpo.py def _build_decouple ( orm_full_scores : torch . Tensor , step_flags : List [ List [ bool ]], step_ids : torch . Tensor , group_ids : torch . Tensor , hyper : \"PRMHyper\" ) -> Tuple [ List [ List [ float ]], Dict ]: ... The decouple process: Construct Raw PRM Rewards : Maps GOOD / BAD flags to numerical scores (e.g., +/- hyper.fix_base ), creating \"process quality\" rewards. Independent Z-Score Normalization : The crucial step that prevents signal interference: Step-level PRM rewards are normalized using _group_zscore_on_steps \u2192 prm_rewards_std Trajectory-level outcome scores are independently normalized \u2192 orm_scores_std Fuse Rewards : Combines the standardized signals with hyper.alpha controlling the balance between attribution (process) and outcome signals. Output: step_rewards - fused, step-level reward values for each trajectory. Stage 3: Advantage Computation - \"How do we guide policy learning?\" Why we need this: The step-level rewards must be converted to token-level advantages that the PPO/GRPO optimizer can use. This involves computing future reward expectations and mapping them to individual tokens. How it works: Converts step-level rewards into token-level advantage tensors. Key Function : suffix_sum_on_steps (from adca_grpo.py ) Calculates advantage for each step by computing cumulative sum of future rewards (suffix sum). This tells each step \"how much future reward you can expect from this point.\" Key Function : broadcast_step_adv_to_tokens (from adca_grpo.py ) Maps step-level advantages to token level using the step_ids tensor, which tracks which step each token belongs to. All tokens in a step receive that step's advantage value. Final Output: The advantages tensor is written back into the batch object for policy training. Key Parameters & Configuration \u2699\ufe0f Here is a comprehensive list of parameters based on the source code and configuration structure. 1. Global & General Settings enable ( bool ) : Master switch for the module . If true , the attribution and advantage rewriting logic will be executed. enable_adca_metric ( bool ) : Enables additional ADCA monitoring metrics. Recommended to turn on for debugging and analysis. 2. LLM Attribution Service These parameters control the behavior of calling the large language model for GOOD / BAD labeling. evaluation_type ( str ) : The evaluation method. Currently fixed to \"api\" , indicating LLM calls via an API. model ( str ) : The LLM model name used for semantic evaluation, e.g., \"qwen-plus\" . concurrent ( int ) : The number of parallel API requests . Adjust based on your API rate limits; a value between 5-20 is recommended. api_max_retries ( int ) : The maximum number of retries for a failed API request. The default is 200, which is very robust. llm_evaluation_log_dir ( str ) : (Optional) The directory path to save LLM request/response logs. Highly recommended for debugging. skip_type ( str ) : Strategy for skipping LLM calls to save costs. \"skip_small_adv\" (recommended) skips samples with near-zero advantage. \"skip_all_neg\" skips negative-reward samples. \"none\" disables skipping. 3. ADCA-GRPO Core Algorithm ( adca_grpo submodule) These parameters directly influence the reward construction, fusion, and advantage computation. prm_scheme ( str ) : The reward fusion scheme. \"decouple\" is strongly recommended as it normalizes attribution and outcome rewards separately, making it more stable. \"allocation\" is an alternative experimental scheme. do_batch_norm ( bool ) : Whether to perform group-wise Z-Score normalization on step rewards. Strongly recommended to be true as it is key to training stability. equal_trajectory_weight ( bool ) : Whether to treat each trajectory equally during normalization (GRPO). Recommended to be true . If false , all steps are pooled together (GSPO), which can be useful in noisy environments. fix_base ( float ) : (For decouple scheme) The base numerical value to map GOOD / BAD labels to. For example, 0.2 means GOOD =+0.2 and BAD =-0.2. alpha ( float ) : The weighting coefficient for the attribution reward (\u03b1) . This is one of the most important hyperparameters for balancing process quality vs. final outcome. beta ( float ) : The weighting coefficient for the outcome reward (\u03b2) . For simplicity, this is often fixed to 1.0 during experiments. orm_distribution ( str ) : The distribution method for the outcome reward (ORM) . \"last_step\" (recommended) applies it only to the final step, while \"all_steps\" distributes it across all steps. prm_steps ( int ) : Enables attribution advantage for the first N epochs only . This is an effective strategy for cost control, allowing the agent to rely on its learned policy in later training stages. enable_length_normalization ( bool ) : (For decouple scheme only) Whether to enable trajectory length normalization (dividing rewards by sqrt of step count). May help balance contributions from long vs. short trajectories. Default is false . Quick Start & Recommended Configuration \ud83d\ude80 Step 1: Set Environment Variable Before starting, ensure your API key is set in your terminal environment: export DASHSCOPE_API_KEY = \"sk-xxxxxxxxxxxxxxxxxxxxxxxx\" Step 2: Recommended Configuration Here is a more complete and ready-to-use baseline configuration. It uses the stable decouple scheme and includes detailed settings for cost optimization and monitoring. # Semantic evaluation & ADCA-GRPO attribution_driven_credit_assignment : # 1. Master switch enable : true # 2. LLM Attribution Service settings evaluation_type : \"api\" model : \"qwen-max\" # Recommend using a powerful model concurrent : 10 # Adjust based on your API QPS limits api_max_retries : 200 llm_evaluation_log_dir : \"/path/to/your/logs\" # Strongly recommended for debugging # 3. ADCA-GRPO Core Algorithm settings adca_grpo : # Must be \"decouple\" for the most stable and validated scheme prm_scheme : \"decouple\" # Must be true to ensure signal scale consistency and training stability do_batch_norm : true # Generally recommended to be true to ensure equal contribution from each trajectory equal_trajectory_weight : true # Base value for GOOD/BAD labels under the \"decouple\" scheme fix_base : 0.2 # Key hyperparameter to tune; start with a small value (e.g., 0.05 ~ 0.2) alpha : 0.1 # This corresponds to '\u03b1' in the paper # Beta is implicitly 1.0 in the code and fixed in experiments # beta: 1.0 # Recommended to be \"last_step\" as it is more intuitive orm_distribution : \"last_step\" # Balance effectiveness and API cost; enables attribution for the first 20 epochs prm_steps : 20 # Recommended to save API costs by not evaluating low-value trajectories skip_type : \"skip_small_adv\" # Recommended to be true to monitor the attribution module's internal state enable_adca_metric : true # Default is false; can be enabled for tasks with high variance in trajectory length enable_length_normalization : false","title":"Advantage Processor"},{"location":"guidelines/adv_processor/#overview","text":"The Advantage Processor is one of core components of the AgentEvolver framework, responsible for implementing the self-attributing mechanism. Traditional reinforcement learning methods face the credit assignment problem in long-horizon tasks: they cannot distinguish which actions in a trajectory are valuable versus unproductive, leading to inefficient learning. The Self-Attributing mechanism addresses this by leveraging LLM's causal reasoning capabilities to decompose learning signals into process quality (logical correctness of actions) and outcome effectiveness (final success), enabling precise step-wise credit assignment.","title":"Overview"},{"location":"guidelines/adv_processor/#core-implementation-adca-grpo","text":"The primary implementation of the Advantage Processor is ADCA-GRPO . ADCA-GRPO uses a powerful LLM to perform causal analysis on trajectories, assigning an attribution signal (GOOD/BAD) to each step. This signal is then combined with the traditional outcome signal to produce a fine-grained advantage, leading to significantly improved sample efficiency. Experimental results demonstrate that ADCA-GRPO achieves approximately 10% performance improvement and 40% reduction in training steps compared to traditional GRPO methods. The modular design allows for future extensions with alternative credit assignment paradigms.","title":"Core Implementation: ADCA-GRPO"},{"location":"guidelines/adv_processor/#adca-grpo-implementation-workflow","text":"The ADCA-GRPO advantage calculation is an end-to-end pipeline that transforms raw trajectory data into a fine-grained advantage signal. Think of it as teaching the agent to distinguish \"good reasoning steps\" from \"bad ones\" - similar to how a teacher evaluates both the problem-solving process and the final answer.","title":"ADCA-GRPO: Implementation Workflow"},{"location":"guidelines/adv_processor/#stage-1-semantic-evaluation-was-this-step-done-correctly","text":"Why we need this: Traditional RL only knows if the final outcome was good or bad, but can't tell which intermediate steps were helpful. We need an LLM to act as a \"step-by-step evaluator.\" Example: In an AppWorld task, the LLM might evaluate \"calling the wrong API\" as BAD, while \"correctly parsing parameters\" gets marked as GOOD. How it works: The system uses an LLM to generate step-wise GOOD / BAD labels for each trajectory in the batch. Key Function : evaluate_step_flags_parallel_sync (from semantic_attribution.py ) # From: semantic_attribution.py async def evaluate_step_flags_parallel ( tokenizer , batch , overall_score_source : str = \"advantages\" , model_name : str = \"qwen-max\" , # ... other parameters ) -> Tuple [ List [ List [ bool ]], Dict ]: ... This function orchestrates the evaluation process: Preparing Data : Decodes prompts and responses for each sample in the batch . Constructing Prompts : Builds detailed prompts for the LLM including the query, full rollout, and final outcome score. Parallel API Calls : Manages asynchronous, parallel API calls with rate limiting and retries via _async_safe_query . Parsing Results : Uses parse_batch_evaluation_result to convert LLM's natural language response into structured boolean flags ( True for GOOD , False for BAD ). Output: step_flags - a list of lists containing boolean labels for each step in every trajectory.","title":"Stage 1: Semantic Evaluation - \"Was this step done correctly?\""},{"location":"guidelines/adv_processor/#stage-2-signal-fusion-how-do-we-balance-process-vs-outcome","text":"Why we need this: Just like evaluating a student's work, we need to consider both the problem-solving steps (process) AND the final answer (outcome). The key insight is to normalize these signals independently to prevent one from overwhelming the other. How it works: With the step_flags , the pipeline calculates the final advantage signal by fusing attribution and outcome signals. Key Function : compute_prm_grpo_advantages (from adca_grpo.py ) # From: adca_grpo.py def compute_prm_grpo_advantages ( batch , step_flags : List [ List [ bool ]], hyper : Optional [ PRMHyper ] = None , scheme : str = \"decouple\" , ) -> dict : ... For the recommended \"decouple\" scheme, this calls _build_decouple , which implements the core logic: Utility Function : _build_decouple (from adca_grpo.py ) # From: adca_grpo.py def _build_decouple ( orm_full_scores : torch . Tensor , step_flags : List [ List [ bool ]], step_ids : torch . Tensor , group_ids : torch . Tensor , hyper : \"PRMHyper\" ) -> Tuple [ List [ List [ float ]], Dict ]: ... The decouple process: Construct Raw PRM Rewards : Maps GOOD / BAD flags to numerical scores (e.g., +/- hyper.fix_base ), creating \"process quality\" rewards. Independent Z-Score Normalization : The crucial step that prevents signal interference: Step-level PRM rewards are normalized using _group_zscore_on_steps \u2192 prm_rewards_std Trajectory-level outcome scores are independently normalized \u2192 orm_scores_std Fuse Rewards : Combines the standardized signals with hyper.alpha controlling the balance between attribution (process) and outcome signals. Output: step_rewards - fused, step-level reward values for each trajectory.","title":"Stage 2: Signal Fusion - \"How do we balance process vs. outcome?\""},{"location":"guidelines/adv_processor/#stage-3-advantage-computation-how-do-we-guide-policy-learning","text":"Why we need this: The step-level rewards must be converted to token-level advantages that the PPO/GRPO optimizer can use. This involves computing future reward expectations and mapping them to individual tokens. How it works: Converts step-level rewards into token-level advantage tensors. Key Function : suffix_sum_on_steps (from adca_grpo.py ) Calculates advantage for each step by computing cumulative sum of future rewards (suffix sum). This tells each step \"how much future reward you can expect from this point.\" Key Function : broadcast_step_adv_to_tokens (from adca_grpo.py ) Maps step-level advantages to token level using the step_ids tensor, which tracks which step each token belongs to. All tokens in a step receive that step's advantage value. Final Output: The advantages tensor is written back into the batch object for policy training.","title":"Stage 3: Advantage Computation - \"How do we guide policy learning?\""},{"location":"guidelines/adv_processor/#key-parameters-configuration","text":"Here is a comprehensive list of parameters based on the source code and configuration structure.","title":"Key Parameters &amp; Configuration \u2699\ufe0f"},{"location":"guidelines/adv_processor/#1-global-general-settings","text":"enable ( bool ) : Master switch for the module . If true , the attribution and advantage rewriting logic will be executed. enable_adca_metric ( bool ) : Enables additional ADCA monitoring metrics. Recommended to turn on for debugging and analysis.","title":"1. Global &amp; General Settings"},{"location":"guidelines/adv_processor/#2-llm-attribution-service","text":"These parameters control the behavior of calling the large language model for GOOD / BAD labeling. evaluation_type ( str ) : The evaluation method. Currently fixed to \"api\" , indicating LLM calls via an API. model ( str ) : The LLM model name used for semantic evaluation, e.g., \"qwen-plus\" . concurrent ( int ) : The number of parallel API requests . Adjust based on your API rate limits; a value between 5-20 is recommended. api_max_retries ( int ) : The maximum number of retries for a failed API request. The default is 200, which is very robust. llm_evaluation_log_dir ( str ) : (Optional) The directory path to save LLM request/response logs. Highly recommended for debugging. skip_type ( str ) : Strategy for skipping LLM calls to save costs. \"skip_small_adv\" (recommended) skips samples with near-zero advantage. \"skip_all_neg\" skips negative-reward samples. \"none\" disables skipping.","title":"2. LLM Attribution Service"},{"location":"guidelines/adv_processor/#3-adca-grpo-core-algorithm-adca_grpo-submodule","text":"These parameters directly influence the reward construction, fusion, and advantage computation. prm_scheme ( str ) : The reward fusion scheme. \"decouple\" is strongly recommended as it normalizes attribution and outcome rewards separately, making it more stable. \"allocation\" is an alternative experimental scheme. do_batch_norm ( bool ) : Whether to perform group-wise Z-Score normalization on step rewards. Strongly recommended to be true as it is key to training stability. equal_trajectory_weight ( bool ) : Whether to treat each trajectory equally during normalization (GRPO). Recommended to be true . If false , all steps are pooled together (GSPO), which can be useful in noisy environments. fix_base ( float ) : (For decouple scheme) The base numerical value to map GOOD / BAD labels to. For example, 0.2 means GOOD =+0.2 and BAD =-0.2. alpha ( float ) : The weighting coefficient for the attribution reward (\u03b1) . This is one of the most important hyperparameters for balancing process quality vs. final outcome. beta ( float ) : The weighting coefficient for the outcome reward (\u03b2) . For simplicity, this is often fixed to 1.0 during experiments. orm_distribution ( str ) : The distribution method for the outcome reward (ORM) . \"last_step\" (recommended) applies it only to the final step, while \"all_steps\" distributes it across all steps. prm_steps ( int ) : Enables attribution advantage for the first N epochs only . This is an effective strategy for cost control, allowing the agent to rely on its learned policy in later training stages. enable_length_normalization ( bool ) : (For decouple scheme only) Whether to enable trajectory length normalization (dividing rewards by sqrt of step count). May help balance contributions from long vs. short trajectories. Default is false .","title":"3. ADCA-GRPO Core Algorithm (adca_grpo submodule)"},{"location":"guidelines/adv_processor/#quick-start-recommended-configuration","text":"","title":"Quick Start &amp; Recommended Configuration \ud83d\ude80"},{"location":"guidelines/adv_processor/#step-1-set-environment-variable","text":"Before starting, ensure your API key is set in your terminal environment: export DASHSCOPE_API_KEY = \"sk-xxxxxxxxxxxxxxxxxxxxxxxx\"","title":"Step 1: Set Environment Variable"},{"location":"guidelines/adv_processor/#step-2-recommended-configuration","text":"Here is a more complete and ready-to-use baseline configuration. It uses the stable decouple scheme and includes detailed settings for cost optimization and monitoring. # Semantic evaluation & ADCA-GRPO attribution_driven_credit_assignment : # 1. Master switch enable : true # 2. LLM Attribution Service settings evaluation_type : \"api\" model : \"qwen-max\" # Recommend using a powerful model concurrent : 10 # Adjust based on your API QPS limits api_max_retries : 200 llm_evaluation_log_dir : \"/path/to/your/logs\" # Strongly recommended for debugging # 3. ADCA-GRPO Core Algorithm settings adca_grpo : # Must be \"decouple\" for the most stable and validated scheme prm_scheme : \"decouple\" # Must be true to ensure signal scale consistency and training stability do_batch_norm : true # Generally recommended to be true to ensure equal contribution from each trajectory equal_trajectory_weight : true # Base value for GOOD/BAD labels under the \"decouple\" scheme fix_base : 0.2 # Key hyperparameter to tune; start with a small value (e.g., 0.05 ~ 0.2) alpha : 0.1 # This corresponds to '\u03b1' in the paper # Beta is implicitly 1.0 in the code and fixed in experiments # beta: 1.0 # Recommended to be \"last_step\" as it is more intuitive orm_distribution : \"last_step\" # Balance effectiveness and API cost; enables attribution for the first 20 epochs prm_steps : 20 # Recommended to save API costs by not evaluating low-value trajectories skip_type : \"skip_small_adv\" # Recommended to be true to monitor the attribution module's internal state enable_adca_metric : true # Default is false; can be enabled for tasks with high variance in trajectory length enable_length_normalization : false","title":"Step 2: Recommended Configuration"},{"location":"guidelines/context_manager/","text":"","title":"Context manager"},{"location":"guidelines/env_service/","text":"To evolve agents in a data-driven manner within dynamic environments, a robust and scalable environment management layer is essential. EnvService fulfills this role by providing a unified service that manages the lifecycle of training environment instances , enabling seamless interaction, evaluation, and control during agent training. EnvService is designed to operate independently of other components and can be launched and used in isolation. It exposes a well-structured API and integrates with distributed execution frameworks like Ray , enabling support for large-scale agent evolution across heterogeneous environments. Core Responsibilities of EnvService Dynamic Environment Instantiation Dynamically create environment instances based on task specifications, parameters, and environment types (e.g., appworld , bfcl , webshop ). Lifecycle and Resource Management Track environment usage and automatically release inactive instances to ensure efficient resource utilization. Unified Interaction Interface Provide a RESTful API for interacting with environments \u2014 including initialization, stepping, evaluation, and instance cleanup. Modular Environment Loading Support dynamic import and registration of custom environment modules without modifying core service logic. Asynchronous and Distributed Execution Leverage Ray actors to run each environment instance independently, enabling scalable and concurrent execution. EnvService serves as the execution backbone for the agent evolution pipeline, enabling consistent interaction patterns, efficient resource handling, and flexible integration with various synthetic or user-defined environments. Environment Setup We provide a simple setup script and a Dockerfile to help you get started quickly. In this subsection, we explain the environment installation process. The launch instructions are provided in the next section. More environments will be supported soon. Appworld To install the appworld environment, navigate to the following directory and run the setup script. This script will configure environment variables, create a Conda environment with Python 3.11, install necessary dependencies, initialize Appworld, and download required data: cd env_service/environments/appworld bash setup.sh BFCL To install the BFCL environment, navigate to the following directory and run the setup script. This script will configure environment variables, create a Conda environment with Python 3.11, install necessary dependencies, initialize BFCL, and download required data: cd env_service/environments/bfcl bash setup.sh Launch Environment Service After installation, you can launch Environment Service with the following commands: Appworld To launch the appworld environment service, run the script in the env_service/launch_script/appworld.sh : source $( conda info --base ) /etc/profile.d/conda.sh conda activate appworld cd env_service/launch_script bash appworld.sh By default, the service will start on 127.0.0.1:8080 . If you need to change the host ( --portal ) or port ( --port ), edit the following line inside env_service/launch_script/appworld.sh : exec python -m env_service.env_service --env appworld --portal 127 .0.0.1 --port 8080 Feel free to replace the IP address or port number as needed to suit your environment. BFCL To launch the bfcl environment service, run the script in the env_service/launch_script/bfcl.sh : source $( conda info --base ) /etc/profile.d/conda.sh conda activate bfcl cd env_service/launch_script bash bfcl.sh By default, the service will start on 127.0.0.1:8080 . If you need to change the host ( --portal ) or port ( --port ), edit the following line inside env_service/launch_script/bfcl.sh : exec python -m env_service.env_service --env bfcl --portal 127 .0.0.1 --port 8080 Feel free to replace the IP address or port number as needed to suit your environment. Environment Service Interface You can also launch the environment service manually without using a script by running: python -m env_service.env_service [ --args ] Here are the available command-line arguments: Argument Type Default Description --env str \"appworld\" The name of the environment to run. --env_file_name str None Optional. Specific file name of the environment module. Defaults to \"{env}_env.py\" . --portal str \"0.0.0.0\" The IP address to bind the server to. Use \"127.0.0.1\" for local only. --port int 8000 Port number to run the server on. --debug bool False Whether to run the server in debug mode ( True ) or production mode ( False ), where useless logger will be removed in production mode. Example Start the server for appworld on port 8080 and localhost: python -m env_service.env_service --env appworld --portal 127 .0.0.1 --port 8080 --debug True This gives you more flexibility to control and integrate the environment service in your own workflows. API Interaction Flow Our EnvService communicates via HTTP requests, primarily through the EnvClient located at env_service/env_client.py , which handles the service connection and has already been integrated into AgentEvolver . Query Task Candidates (Optional): POST /get_env_profile \u2192 Returns a list of available task_id s for a given environment type. Create Instance: POST /create \u2192 Initializes a new environment with env_type , task_id , and custom parameters. \u2192 Returns the initial state. Interact with Environment: POST /step \u2192 Sends an action to the environment instance. \u2192 Returns the next state, reward, and done signal. Evaluate Agent Behavior (Optional): POST /evaluate \u2192 Used for scoring overall task performance (e.g., episodic evaluation). Get Instance Information (Optional): POST /get_info \u2192 Retrieve metadata or progress information from the environment. Release Instance: POST /release \u2192 Frees the environment resources explicitly. Health Check: GET /healthz \u2192 Basic service status check. Input & Output Data Structure To help users interact effectively with the Appworld environment through the EnvService API, this section details the expected input and output data structures used in the /create and /step calls. These examples illustrate what kind of requests to send and how to interpret the returned results. More examples can be found in env_service/interface.md \ud83d\udee0\ufe0f /create \u2013 Environment Initialization Description: Initializes a new environment instance and prepares its initial state. \ud83d\udd3d Input Parameters { \"env\" : \"appworld\" , \"instance_id\" : \"my-instance-id-001\" , \"task_id\" : \"task-001\" , \"params\" : { \"simple\" : false , \"prompt\" : true } } Field Type Description env string The environment to use. Set to \"appworld\" . instance_id string A user-defined identifier for tracking the environment instance. task_id string The task assigned to this instance. Tasks can differ by difficulty or type. params dict Additional configuration options: simple bool \u2013 If true, uses a simplified interaction format. prompt bool \u2013 If true, includes the environment prompt template in the state. \u23ce Output (Initial State) { \"state\" : [ { \"role\" : \"system\" , \"content\" : \"[Structured prompt with task description and tool info]\" }, { \"role\" : \"user\" , \"content\" : \"[Task instruction from Appworld]\" } ], \"info\" : { \"instance_id\" : \"my-instance-id-001\" , \"task_id\" : \"task-001\" } } \ud83d\udd04 /step \u2013 Sending an Action to the Environment Description: Advances the environment one step using a user-generated action. \ud83d\udd3d Input Format { \"action\" : { \"role\" : \"assistant\" , \"content\" : \"```python\\nopen_app(\\\"Calendar\\\")\\n```\" , \"tool_calls\" : [] } } Field Type Description role str Typically \"assistant\" , representing the agent performing the action. content str The agent's natural language or code action. May include Python code blocks. tool_calls list Optional list of structured tool calls (used for advanced interactions). \u2705 You can omit tool_calls if your action is just a plain code string. \ud83d\udce4 Response from /step { \"state\" : [ { \"role\" : \"assistant\" , \"content\" : \"Output:\\n```\\nApp opened successfully\\n```\" } ], \"reward\" : 1.0 , \"is_terminated\" : true , \"info\" : {} } Field Type Description state list Environment\u2019s response, typically including tool output or result summary. reward float Task performance score. 1.0 = success, 0.0 = failure (configurable). is_terminated bool Indicates whether the task is completed. info dict Reserved for additional metadata (currently empty). \ud83d\udca1 Tips for Use The content field should contain a well-formatted Python action or a command interpretable by Appworld. You may use Markdown-style code blocks ( ```python ) to wrap executable code for better readability. If is_terminated: true , the environment expects no further actions. Use the reward to evaluate success in training or evaluation settings. The simple param in /create makes the prompt and output shorter \u2014 useful for debugging or fast testing. For more details on tool calls, prompt generation, or evaluation metrics, refer to the Appworld Environment Internals section. \"\"\"","title":"Environment Service"},{"location":"guidelines/env_service/#core-responsibilities-of-envservice","text":"Dynamic Environment Instantiation Dynamically create environment instances based on task specifications, parameters, and environment types (e.g., appworld , bfcl , webshop ). Lifecycle and Resource Management Track environment usage and automatically release inactive instances to ensure efficient resource utilization. Unified Interaction Interface Provide a RESTful API for interacting with environments \u2014 including initialization, stepping, evaluation, and instance cleanup. Modular Environment Loading Support dynamic import and registration of custom environment modules without modifying core service logic. Asynchronous and Distributed Execution Leverage Ray actors to run each environment instance independently, enabling scalable and concurrent execution. EnvService serves as the execution backbone for the agent evolution pipeline, enabling consistent interaction patterns, efficient resource handling, and flexible integration with various synthetic or user-defined environments.","title":"Core Responsibilities of EnvService"},{"location":"guidelines/env_service/#environment-setup","text":"We provide a simple setup script and a Dockerfile to help you get started quickly. In this subsection, we explain the environment installation process. The launch instructions are provided in the next section. More environments will be supported soon.","title":"Environment Setup"},{"location":"guidelines/env_service/#appworld","text":"To install the appworld environment, navigate to the following directory and run the setup script. This script will configure environment variables, create a Conda environment with Python 3.11, install necessary dependencies, initialize Appworld, and download required data: cd env_service/environments/appworld bash setup.sh","title":"Appworld"},{"location":"guidelines/env_service/#bfcl","text":"To install the BFCL environment, navigate to the following directory and run the setup script. This script will configure environment variables, create a Conda environment with Python 3.11, install necessary dependencies, initialize BFCL, and download required data: cd env_service/environments/bfcl bash setup.sh","title":"BFCL"},{"location":"guidelines/env_service/#launch-environment-service","text":"After installation, you can launch Environment Service with the following commands:","title":"Launch Environment Service"},{"location":"guidelines/env_service/#appworld_1","text":"To launch the appworld environment service, run the script in the env_service/launch_script/appworld.sh : source $( conda info --base ) /etc/profile.d/conda.sh conda activate appworld cd env_service/launch_script bash appworld.sh By default, the service will start on 127.0.0.1:8080 . If you need to change the host ( --portal ) or port ( --port ), edit the following line inside env_service/launch_script/appworld.sh : exec python -m env_service.env_service --env appworld --portal 127 .0.0.1 --port 8080 Feel free to replace the IP address or port number as needed to suit your environment.","title":"Appworld"},{"location":"guidelines/env_service/#bfcl_1","text":"To launch the bfcl environment service, run the script in the env_service/launch_script/bfcl.sh : source $( conda info --base ) /etc/profile.d/conda.sh conda activate bfcl cd env_service/launch_script bash bfcl.sh By default, the service will start on 127.0.0.1:8080 . If you need to change the host ( --portal ) or port ( --port ), edit the following line inside env_service/launch_script/bfcl.sh : exec python -m env_service.env_service --env bfcl --portal 127 .0.0.1 --port 8080 Feel free to replace the IP address or port number as needed to suit your environment.","title":"BFCL"},{"location":"guidelines/env_service/#environment-service-interface","text":"You can also launch the environment service manually without using a script by running: python -m env_service.env_service [ --args ] Here are the available command-line arguments: Argument Type Default Description --env str \"appworld\" The name of the environment to run. --env_file_name str None Optional. Specific file name of the environment module. Defaults to \"{env}_env.py\" . --portal str \"0.0.0.0\" The IP address to bind the server to. Use \"127.0.0.1\" for local only. --port int 8000 Port number to run the server on. --debug bool False Whether to run the server in debug mode ( True ) or production mode ( False ), where useless logger will be removed in production mode.","title":"Environment Service Interface"},{"location":"guidelines/env_service/#example","text":"Start the server for appworld on port 8080 and localhost: python -m env_service.env_service --env appworld --portal 127 .0.0.1 --port 8080 --debug True This gives you more flexibility to control and integrate the environment service in your own workflows.","title":"Example"},{"location":"guidelines/env_service/#api-interaction-flow","text":"Our EnvService communicates via HTTP requests, primarily through the EnvClient located at env_service/env_client.py , which handles the service connection and has already been integrated into AgentEvolver . Query Task Candidates (Optional): POST /get_env_profile \u2192 Returns a list of available task_id s for a given environment type. Create Instance: POST /create \u2192 Initializes a new environment with env_type , task_id , and custom parameters. \u2192 Returns the initial state. Interact with Environment: POST /step \u2192 Sends an action to the environment instance. \u2192 Returns the next state, reward, and done signal. Evaluate Agent Behavior (Optional): POST /evaluate \u2192 Used for scoring overall task performance (e.g., episodic evaluation). Get Instance Information (Optional): POST /get_info \u2192 Retrieve metadata or progress information from the environment. Release Instance: POST /release \u2192 Frees the environment resources explicitly. Health Check: GET /healthz \u2192 Basic service status check.","title":"API Interaction Flow"},{"location":"guidelines/env_service/#input-output-data-structure","text":"To help users interact effectively with the Appworld environment through the EnvService API, this section details the expected input and output data structures used in the /create and /step calls. These examples illustrate what kind of requests to send and how to interpret the returned results. More examples can be found in env_service/interface.md","title":"Input &amp; Output Data Structure"},{"location":"guidelines/env_service/#create-environment-initialization","text":"Description: Initializes a new environment instance and prepares its initial state.","title":"\ud83d\udee0\ufe0f /create \u2013 Environment Initialization"},{"location":"guidelines/env_service/#input-parameters","text":"{ \"env\" : \"appworld\" , \"instance_id\" : \"my-instance-id-001\" , \"task_id\" : \"task-001\" , \"params\" : { \"simple\" : false , \"prompt\" : true } } Field Type Description env string The environment to use. Set to \"appworld\" . instance_id string A user-defined identifier for tracking the environment instance. task_id string The task assigned to this instance. Tasks can differ by difficulty or type. params dict Additional configuration options: simple bool \u2013 If true, uses a simplified interaction format. prompt bool \u2013 If true, includes the environment prompt template in the state.","title":"\ud83d\udd3d Input Parameters"},{"location":"guidelines/env_service/#output-initial-state","text":"{ \"state\" : [ { \"role\" : \"system\" , \"content\" : \"[Structured prompt with task description and tool info]\" }, { \"role\" : \"user\" , \"content\" : \"[Task instruction from Appworld]\" } ], \"info\" : { \"instance_id\" : \"my-instance-id-001\" , \"task_id\" : \"task-001\" } }","title":"\u23ce Output (Initial State)"},{"location":"guidelines/env_service/#step-sending-an-action-to-the-environment","text":"Description: Advances the environment one step using a user-generated action.","title":"\ud83d\udd04 /step \u2013 Sending an Action to the Environment"},{"location":"guidelines/env_service/#input-format","text":"{ \"action\" : { \"role\" : \"assistant\" , \"content\" : \"```python\\nopen_app(\\\"Calendar\\\")\\n```\" , \"tool_calls\" : [] } } Field Type Description role str Typically \"assistant\" , representing the agent performing the action. content str The agent's natural language or code action. May include Python code blocks. tool_calls list Optional list of structured tool calls (used for advanced interactions). \u2705 You can omit tool_calls if your action is just a plain code string.","title":"\ud83d\udd3d Input Format"},{"location":"guidelines/env_service/#response-from-step","text":"{ \"state\" : [ { \"role\" : \"assistant\" , \"content\" : \"Output:\\n```\\nApp opened successfully\\n```\" } ], \"reward\" : 1.0 , \"is_terminated\" : true , \"info\" : {} } Field Type Description state list Environment\u2019s response, typically including tool output or result summary. reward float Task performance score. 1.0 = success, 0.0 = failure (configurable). is_terminated bool Indicates whether the task is completed. info dict Reserved for additional metadata (currently empty).","title":"\ud83d\udce4 Response from /step"},{"location":"guidelines/env_service/#tips-for-use","text":"The content field should contain a well-formatted Python action or a command interpretable by Appworld. You may use Markdown-style code blocks ( ```python ) to wrap executable code for better readability. If is_terminated: true , the environment expects no further actions. Use the reward to evaluate success in training or evaluation settings. The simple param in /create makes the prompt and output shorter \u2014 useful for debugging or fast testing. For more details on tool calls, prompt generation, or evaluation metrics, refer to the Appworld Environment Internals section. \"\"\"","title":"\ud83d\udca1 Tips for Use"},{"location":"guidelines/exp_manager/","text":"\ud83d\ude80 Overview Exploration in complex environments is a key challenge for autonomous agents. Traditional reinforcement learning approaches rely heavily on trial-and-error, often generating redundant trajectories and converging slowly. In contrast, humans efficiently leverage past experiences to guide future actions, learning faster and more systematically. The Self-Navigating framework adopts this principle by enabling agents to internalize and reuse prior experiences. It shifts exploration from unguided trial-and-error to structured, knowledge-driven self-improvement, improving both learning efficiency and policy quality. At the core of the framework, the Experience Manager oversees all aspects of experience handling, including: Experience Pool Management \u2013 Constructing and updating the experience pool with new trajectories and summaries. Experience Mode Control \u2013 Determining whether to add experiences during rollouts and whether to remove experience information during training. Rollout & Training Context Management \u2013 Providing relevant historical context during rollouts and maintaining experience-stripped training messages. Training Loss Management \u2013 Aggregating and processing losses with respect to experience-based adjustments for stable learning. \ud83e\udde9 Core Features The Self-Navigating framework enhances reinforcement learning by transforming how agents create, reuse, and refine experiences . It introduces structured mechanisms that make exploration more efficient , context-aware , and self-evolving . At its core are two classes: ExperienceManager \u2014 handles experience scheduling and allocation. ExperienceWorker \u2014 manages context injection during rollout and cleanup during training. 1. Dynamic Experience Allocation Purpose : Dynamically decide how much and when to use experience during both training and rollout stages. How it works : This module performs two levels of adaptive allocation: Task-Level Allocation Determines whether each training task should keep or discard experience. Controlled by train_sample_mode : \"allkeep\" \u2192 all tasks retain experience \"alldiscard\" \u2192 all tasks discard experience \"hybrid\" \u2192 keep ratio controlled by train_sample_keepratio Key Codes: # Class: ExperienceManager # Function: allocate_train_mode() mode_to_ratio = { \"allkeep\" : 1.0 , \"alldiscard\" : 0.0 , \"hybrid\" : self . train_sample_keepratio } keep_ratio = mode_to_ratio . get ( self . train_sample_mode , self . train_sample_keepratio ) keep_count = int ( len ( tasks ) * keep_ratio ) exp_modes = [ 'keep' ] * keep_count + [ 'discard' ] * ( len ( tasks ) - keep_count ) Rollout-Level Allocation Determines the proportion of rollouts within one task that will include experience . Controlled by val_rollout_mode and train_rollout_mode : \"woexp\" \u2192 no rollout uses experience (pure exploration) \"all\" \u2192 all rollouts use experience (fully guided) \"mixed\" \u2192 partially guided , rollout experience usage ratio determined by rollout_ratio The parameter rollout_ratio only takes effect when val_rollout_mode/train_rollout_mode=\"mixed\" . For example, rollout_expratio=0.3 means 30% of rollouts will include experience, while the remaining 70% proceed without it. # Class: ExperienceManager # Function: allocate_add_exp() add_exp_choices = { \"woexp\" : [ False ] * rollout_n , \"mixed\" : sorted ( [ i < round ( rollout_n * self . rollout_expratio ) for i in range ( rollout_n )], key = lambda _ : random . random () ), \"all\" : [ True ] * rollout_n }[ exp_mode ] \u2705Effect : train_sample_mode controls how experience is used in training samples. val_rollout_mode/train_rollout_mode defines the exploration regime ( woexp / mixed / all ). rollout_ratio refines mixed mode by determining how many rollouts reuse experience. Together, they enable dynamic balancing between exploration and exploitation. 2. Asynchronous Experience Summarization Purpose : Convert raw trajectories into summarized experiences asynchronously , ensuring continuous learning without blocking. How it works : Periodically triggered by training steps ( updated_freq ). Executes summarization jobs via a background thread ( ThreadPoolExecutor ). Stores summarized results in the shared experience pool. # Class: ExperienceManager # FUnction: submit_summary_task() summary_task = self . thread_pool . submit ( self . em_client . call_summarizer , trajectories = trajectories , workspace_id = reme_config . workspace_id ) \u2705Effect : The experience pool grows in parallel with training, keeping the agent\u2019s knowledge base continuously updated. 3. Context-Aware Rollout Management Purpose : Make rollouts context-aware by injecting relevant past experiences into prompts. How it works : Retrieves top-K related experiences via EMClient . Formats and prepends them to the rollout message. Enhances rollout context without modifying the underlying task. # Class: ExperienceWorker # Function: manage_rollout_context() history_experience = self . em_client . call_context_generator ( trajectory = trajectory , retrieve_top_k = reme_config . retrieve_top_k , workspace_id = reme_config . workspace_id ) formatted_experience = self . experience_template . format ( history_experience ) new_content = formatted_experience + trajectory . steps [ - 1 ][ \"content\" ] trajectory . steps [ - 1 ][ \"content\" ] = new_content \u2705Effect : Each rollout benefits from relevant prior knowledge, reducing redundant exploration. 4. Training Context Management Purpose : Ensure training messages remain clean by removing injected experience when not needed. How it works : Detects experience templates in training messages using regex. Removes them when train_mode=\"discard\" while retaining extracted text for analysis. # Class: ExperienceWorker # Function: manage_training_context() pattern = re . escape ( self . experience_template ) . replace ( r '\\{\\}' , '(.*?)' ) cleaned_message = re . sub ( pattern , '' , message , flags = re . DOTALL ) \u2705Effect : Guarantees that training data integrity aligns with the current experience policy. 5. Training Loss Processing Purpose : Ensure stable policy updates when mixing on-policy rollouts and off-policy experience replays , allowing the agent to leverage past trajectories without destabilizing learning. How it Works : This module computes a heterogeneous PPO loss that combines: On-policy loss : derived from fresh rollouts. Off-policy loss : derived from experience-augmented samples. An experience mask ( exp_mask ) distinguishes the two. Each loss is clipped and optionally adjusted for negative advantages. Finally, the losses are combined and aggregated according to loss_agg_mode . # Function: het_compute_token_on_off_policy_loss() # 1\ufe0f\u20e3 Compute policy ratio and approximate KL divergence negative_approx_kl = log_prob - old_log_prob # difference between new and old log-probabilities ratio = torch . exp ( negative_approx_kl ) # policy ratio r_t = exp(log_pi_new - log_pi_old) ppo_kl = verl_F . masked_mean ( - negative_approx_kl , response_mask ) # approximate KL divergence # 2\ufe0f\u20e3 Compute on-policy losses (exp_mask = 0) on_pg_losses , _ , _ = compute_pg_losses ( cliprange_low , cliprange_high ) # Mask out experience tokens to ensure only fresh rollouts contribute on_pg_loss = verl_F . masked_mean ( on_pg_losses , ( 1.0 - exp_mask ) * response_mask ) # 3\ufe0f\u20e3 Compute off-policy losses (exp_mask = 1) off_pg_losses , _ , _ = compute_pg_losses ( off_cliprange_low , off_cliprange_high ) # Mask to include only experience tokens off_pg_loss = verl_F . masked_mean ( off_pg_losses , exp_mask * response_mask ) # Ensure numerical stability off_pg_loss = torch . tensor ( 0.0 ) if off_pg_loss . isnan () . item () else off_pg_loss # 4\ufe0f\u20e3 Combine both losses using the experience mask pg_losses = off_pg_losses * exp_mask + on_pg_losses * ( 1.0 - exp_mask ) # Aggregate token-level losses according to selected mode (e.g., \"token-mean\") pg_loss = agg_loss ( loss_mat = pg_losses , loss_mask = response_mask , loss_agg_mode = loss_agg_mode ) \u2705Effect : exp_mask separates on-policy and off-policy contributions cleanly. off_cliprange_high enforce trust regions for stable updates. \u2699\ufe0f Key Parameters & Configuration Rollout Modes train_rollout_mode ( str ) : Controls how experiences are used during training rollouts . Options: - \"woexp\" \u2192 rollouts without any experience guidance (pure exploration) - \"mixed\" \u2192 partially inject experiences based on rollout_ratio - \"all\" \u2192 all rollouts include retrieved experiences Default: \"woexp\" . val_rollout_mode ( str ) : Controls how experiences are used during validation/test rollouts . Same options as train_rollout_mode . Typically set to \"woexp\" for unbiased evaluation. Default: \"woexp\" . rollout_ratio ( float , range: [0.0, 1.0]) : When rollout mode is \"mixed\" , this ratio determines the proportion of rollouts that include experience. Example: 0.3 means 30% experience-guided, 70% exploratory. Default: 0.0 . Training Sample Processing train_sample_mode ( str ) : Defines whether to keep or discard experience context in training samples after rollout. Options: - \"allkeep\" \u2192 all samples retain experience information - \"alldiscard\" \u2192 strip experience from all samples (model learns from raw reasoning) - \"hybrid\" \u2192 selective retention based on train_sample_keepratio Default: \"alldiscard\" . train_sample_keepratio ( float , range: [0.0, 1.0]) : When train_sample_mode=\"hybrid\" , controls the task-level proportion of samples that retain experience. Default: 1.0 . Experience Retrieval & Injection experience_template ( str ) : Template string for wrapping retrieved experiences before injecting into prompts. The {} placeholder is replaced with formatted experience content. Example: \"\\n\\nSome Related Experience to help you to complete the task:<EXP>{}</EXP>\\n\\n\" retrieve_top_k ( int ) : Number of most relevant experiences to retrieve per query when enable_context_generator=True . Default: 3 . ReMe Service Configuration base_url ( str ) : HTTP endpoint of the ReMe service (Reflective Memory Engine). Handles experience summarization, storage, and retrieval. Example: \"http://127.0.0.1:8001\" . workspace_id ( str ) : Unique identifier for the experience workspace. Different workspaces maintain isolated experience pools. Default: \"default\" . enable_summarizer ( bool ) : Whether to activate automatic experience summarization from rollout trajectories. When True , raw reasoning traces are distilled into reusable experience snippets. Default: False . enable_context_generator ( bool ) : Whether to enable experience retrieval and context injection during rollouts. Must be True for experience-guided rollouts to function. Default: False . updated_freq ( int ) : Frequency (in training steps) to refresh the experience pool. Set to 0 to disable periodic updates. Default: 0 . Initialization & Utilities init_exp_before_training ( bool ) : Whether to pre-populate the experience pool before training begins. Useful for warm-start scenarios with prior knowledge. Default: False . init_exp_only ( bool ) : If True , only initializes the experience pool without starting training. Useful for pre-computing embeddings or validating summarization quality. Default: False . summary_batch_size ( int ) : Batch size for processing experience summarization requests. Larger values improve throughput but require more memory. Default: 8 . \ud83e\udded Quick Start & Recommended Configuration Step 1: Set Up ReMe Service Ensure you have completed the ReMe installation ; Ensure you have modified the environment configuration ; Start the ReMe service with the following command: cd path_to_reme/ReMe reme \\ config = default \\ backend = http \\ thread_pool_max_workers = 256 \\ http.host = \"127.0.0.1\" \\ http.port = 8001 \\ http.limit_concurrency = 256 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local \\ op.rerank_memory_op.params.enable_llm_rerank = false \\ flow.summary_task_memory.flow_content = \"trajectory_preprocess_op >> (success_extraction_op|failure_extraction_op|comparative_extraction_op) >> memory_validation_op >> memory_deduplication_op >> update_vector_store_op\" Step 2: Recommended Configuration Add the following configuration to your experiment settings: exp_manager : val_rollout_mode : \"woexp\" # rollout mode on dev/test set: [\"mixed\", \"all\", \"woexp\"] train_rollout_mode : \"mixed\" # rollout mode on train set: [\"mixed\", \"all\", \"woexp\"] rollout_ratio : 0.5 # ratio of adding experience within group during rollout train_sample_mode : \"alldiscard\" # sample mode after train rollout: [\"allkeep\", \"alldiscard\", \"hybrid\"] train_sample_keepratio : 0.0 # task-level ratio for keeping experience experience_template : \"\\n\\nSome Related Experience to help you to complete the task:<EXP>{}</EXP>\\n\\n\" # template for inserting experience init_exp_before_training : True # whether to init experience pool before training init_exp_only : False # whether to only init experience pool summary_batch_size : 8 # batch size for experience summarization reme : base_url : \"http://127.0.0.1:8001\" # base URL for ReMe service workspace_id : \"default\" # workspace ID for ReMe enable_summarizer : True # whether to enable experience summarizer enable_context_generator : True # whether to enable context generator retrieve_top_k : 3 # number of top experiences to retrieve updated_freq : 0 # experience pool update frequency (in steps, 0=disabled) Step 3: Understanding Different Usage Modes The experience pool system supports multiple usage modes to fit different experimental needs. Configure the parameters according to the table below: Mode Descriptions Usage Mode Description No Pool Training without any experience pool (baseline mode) Init Only Initialize experience pool from trajectories only, without using it during training Init + Train Initialize experience pool before training and use it throughout the training process Init + Train + Update Initialize experience pool, use during training, and continuously update it with new experiences Exist + Train Use an existing experience pool during training without initialization or updates Exist + Train + Update Use an existing experience pool during training and continuously update it with new experiences Configuration Matrix No Pool Init Only Init + Train Init + Train + Update Exist + Train Exist + Train + Update updated_freq 0 0 0 k != 0 0 k != 0 init_exp_only False True False False False False init_exp_before_training False True True True False False enable_summarizer False True False True False True enable_context_generator False False True True True True workspace_id - New ID New ID New ID Exist ID Exist ID Configuration Notes updated_freq : Set to a non-zero value (e.g., 100 ) to enable periodic experience pool updates during training. 0 disables updates. workspace_id : Use a new workspace ID to create a fresh experience pool Specify an existing workspace ID to reuse a previously created pool When using vector_store.default.backend=local , the experience pool is saved at: ReMe/local_vector_store/{workspace_id}.jsonl Recommended starting point : For most use cases, we recommend starting with Init + Train mode.","title":"Experience Manager"},{"location":"guidelines/exp_manager/#overview","text":"Exploration in complex environments is a key challenge for autonomous agents. Traditional reinforcement learning approaches rely heavily on trial-and-error, often generating redundant trajectories and converging slowly. In contrast, humans efficiently leverage past experiences to guide future actions, learning faster and more systematically. The Self-Navigating framework adopts this principle by enabling agents to internalize and reuse prior experiences. It shifts exploration from unguided trial-and-error to structured, knowledge-driven self-improvement, improving both learning efficiency and policy quality. At the core of the framework, the Experience Manager oversees all aspects of experience handling, including: Experience Pool Management \u2013 Constructing and updating the experience pool with new trajectories and summaries. Experience Mode Control \u2013 Determining whether to add experiences during rollouts and whether to remove experience information during training. Rollout & Training Context Management \u2013 Providing relevant historical context during rollouts and maintaining experience-stripped training messages. Training Loss Management \u2013 Aggregating and processing losses with respect to experience-based adjustments for stable learning.","title":"\ud83d\ude80 Overview"},{"location":"guidelines/exp_manager/#core-features","text":"The Self-Navigating framework enhances reinforcement learning by transforming how agents create, reuse, and refine experiences . It introduces structured mechanisms that make exploration more efficient , context-aware , and self-evolving . At its core are two classes: ExperienceManager \u2014 handles experience scheduling and allocation. ExperienceWorker \u2014 manages context injection during rollout and cleanup during training.","title":"\ud83e\udde9 Core Features"},{"location":"guidelines/exp_manager/#1-dynamic-experience-allocation","text":"Purpose : Dynamically decide how much and when to use experience during both training and rollout stages. How it works : This module performs two levels of adaptive allocation: Task-Level Allocation Determines whether each training task should keep or discard experience. Controlled by train_sample_mode : \"allkeep\" \u2192 all tasks retain experience \"alldiscard\" \u2192 all tasks discard experience \"hybrid\" \u2192 keep ratio controlled by train_sample_keepratio Key Codes: # Class: ExperienceManager # Function: allocate_train_mode() mode_to_ratio = { \"allkeep\" : 1.0 , \"alldiscard\" : 0.0 , \"hybrid\" : self . train_sample_keepratio } keep_ratio = mode_to_ratio . get ( self . train_sample_mode , self . train_sample_keepratio ) keep_count = int ( len ( tasks ) * keep_ratio ) exp_modes = [ 'keep' ] * keep_count + [ 'discard' ] * ( len ( tasks ) - keep_count ) Rollout-Level Allocation Determines the proportion of rollouts within one task that will include experience . Controlled by val_rollout_mode and train_rollout_mode : \"woexp\" \u2192 no rollout uses experience (pure exploration) \"all\" \u2192 all rollouts use experience (fully guided) \"mixed\" \u2192 partially guided , rollout experience usage ratio determined by rollout_ratio The parameter rollout_ratio only takes effect when val_rollout_mode/train_rollout_mode=\"mixed\" . For example, rollout_expratio=0.3 means 30% of rollouts will include experience, while the remaining 70% proceed without it. # Class: ExperienceManager # Function: allocate_add_exp() add_exp_choices = { \"woexp\" : [ False ] * rollout_n , \"mixed\" : sorted ( [ i < round ( rollout_n * self . rollout_expratio ) for i in range ( rollout_n )], key = lambda _ : random . random () ), \"all\" : [ True ] * rollout_n }[ exp_mode ] \u2705Effect : train_sample_mode controls how experience is used in training samples. val_rollout_mode/train_rollout_mode defines the exploration regime ( woexp / mixed / all ). rollout_ratio refines mixed mode by determining how many rollouts reuse experience. Together, they enable dynamic balancing between exploration and exploitation.","title":"1. Dynamic Experience Allocation"},{"location":"guidelines/exp_manager/#2-asynchronous-experience-summarization","text":"Purpose : Convert raw trajectories into summarized experiences asynchronously , ensuring continuous learning without blocking. How it works : Periodically triggered by training steps ( updated_freq ). Executes summarization jobs via a background thread ( ThreadPoolExecutor ). Stores summarized results in the shared experience pool. # Class: ExperienceManager # FUnction: submit_summary_task() summary_task = self . thread_pool . submit ( self . em_client . call_summarizer , trajectories = trajectories , workspace_id = reme_config . workspace_id ) \u2705Effect : The experience pool grows in parallel with training, keeping the agent\u2019s knowledge base continuously updated.","title":"2. Asynchronous Experience Summarization"},{"location":"guidelines/exp_manager/#3-context-aware-rollout-management","text":"Purpose : Make rollouts context-aware by injecting relevant past experiences into prompts. How it works : Retrieves top-K related experiences via EMClient . Formats and prepends them to the rollout message. Enhances rollout context without modifying the underlying task. # Class: ExperienceWorker # Function: manage_rollout_context() history_experience = self . em_client . call_context_generator ( trajectory = trajectory , retrieve_top_k = reme_config . retrieve_top_k , workspace_id = reme_config . workspace_id ) formatted_experience = self . experience_template . format ( history_experience ) new_content = formatted_experience + trajectory . steps [ - 1 ][ \"content\" ] trajectory . steps [ - 1 ][ \"content\" ] = new_content \u2705Effect : Each rollout benefits from relevant prior knowledge, reducing redundant exploration.","title":"3. Context-Aware Rollout Management"},{"location":"guidelines/exp_manager/#4-training-context-management","text":"Purpose : Ensure training messages remain clean by removing injected experience when not needed. How it works : Detects experience templates in training messages using regex. Removes them when train_mode=\"discard\" while retaining extracted text for analysis. # Class: ExperienceWorker # Function: manage_training_context() pattern = re . escape ( self . experience_template ) . replace ( r '\\{\\}' , '(.*?)' ) cleaned_message = re . sub ( pattern , '' , message , flags = re . DOTALL ) \u2705Effect : Guarantees that training data integrity aligns with the current experience policy.","title":"4. Training Context Management"},{"location":"guidelines/exp_manager/#5-training-loss-processing","text":"Purpose : Ensure stable policy updates when mixing on-policy rollouts and off-policy experience replays , allowing the agent to leverage past trajectories without destabilizing learning. How it Works : This module computes a heterogeneous PPO loss that combines: On-policy loss : derived from fresh rollouts. Off-policy loss : derived from experience-augmented samples. An experience mask ( exp_mask ) distinguishes the two. Each loss is clipped and optionally adjusted for negative advantages. Finally, the losses are combined and aggregated according to loss_agg_mode . # Function: het_compute_token_on_off_policy_loss() # 1\ufe0f\u20e3 Compute policy ratio and approximate KL divergence negative_approx_kl = log_prob - old_log_prob # difference between new and old log-probabilities ratio = torch . exp ( negative_approx_kl ) # policy ratio r_t = exp(log_pi_new - log_pi_old) ppo_kl = verl_F . masked_mean ( - negative_approx_kl , response_mask ) # approximate KL divergence # 2\ufe0f\u20e3 Compute on-policy losses (exp_mask = 0) on_pg_losses , _ , _ = compute_pg_losses ( cliprange_low , cliprange_high ) # Mask out experience tokens to ensure only fresh rollouts contribute on_pg_loss = verl_F . masked_mean ( on_pg_losses , ( 1.0 - exp_mask ) * response_mask ) # 3\ufe0f\u20e3 Compute off-policy losses (exp_mask = 1) off_pg_losses , _ , _ = compute_pg_losses ( off_cliprange_low , off_cliprange_high ) # Mask to include only experience tokens off_pg_loss = verl_F . masked_mean ( off_pg_losses , exp_mask * response_mask ) # Ensure numerical stability off_pg_loss = torch . tensor ( 0.0 ) if off_pg_loss . isnan () . item () else off_pg_loss # 4\ufe0f\u20e3 Combine both losses using the experience mask pg_losses = off_pg_losses * exp_mask + on_pg_losses * ( 1.0 - exp_mask ) # Aggregate token-level losses according to selected mode (e.g., \"token-mean\") pg_loss = agg_loss ( loss_mat = pg_losses , loss_mask = response_mask , loss_agg_mode = loss_agg_mode ) \u2705Effect : exp_mask separates on-policy and off-policy contributions cleanly. off_cliprange_high enforce trust regions for stable updates.","title":"5. Training Loss Processing"},{"location":"guidelines/exp_manager/#key-parameters-configuration","text":"","title":"\u2699\ufe0f Key Parameters &amp; Configuration"},{"location":"guidelines/exp_manager/#rollout-modes","text":"train_rollout_mode ( str ) : Controls how experiences are used during training rollouts . Options: - \"woexp\" \u2192 rollouts without any experience guidance (pure exploration) - \"mixed\" \u2192 partially inject experiences based on rollout_ratio - \"all\" \u2192 all rollouts include retrieved experiences Default: \"woexp\" . val_rollout_mode ( str ) : Controls how experiences are used during validation/test rollouts . Same options as train_rollout_mode . Typically set to \"woexp\" for unbiased evaluation. Default: \"woexp\" . rollout_ratio ( float , range: [0.0, 1.0]) : When rollout mode is \"mixed\" , this ratio determines the proportion of rollouts that include experience. Example: 0.3 means 30% experience-guided, 70% exploratory. Default: 0.0 .","title":"Rollout Modes"},{"location":"guidelines/exp_manager/#training-sample-processing","text":"train_sample_mode ( str ) : Defines whether to keep or discard experience context in training samples after rollout. Options: - \"allkeep\" \u2192 all samples retain experience information - \"alldiscard\" \u2192 strip experience from all samples (model learns from raw reasoning) - \"hybrid\" \u2192 selective retention based on train_sample_keepratio Default: \"alldiscard\" . train_sample_keepratio ( float , range: [0.0, 1.0]) : When train_sample_mode=\"hybrid\" , controls the task-level proportion of samples that retain experience. Default: 1.0 .","title":"Training Sample Processing"},{"location":"guidelines/exp_manager/#experience-retrieval-injection","text":"experience_template ( str ) : Template string for wrapping retrieved experiences before injecting into prompts. The {} placeholder is replaced with formatted experience content. Example: \"\\n\\nSome Related Experience to help you to complete the task:<EXP>{}</EXP>\\n\\n\" retrieve_top_k ( int ) : Number of most relevant experiences to retrieve per query when enable_context_generator=True . Default: 3 .","title":"Experience Retrieval &amp; Injection"},{"location":"guidelines/exp_manager/#reme-service-configuration","text":"base_url ( str ) : HTTP endpoint of the ReMe service (Reflective Memory Engine). Handles experience summarization, storage, and retrieval. Example: \"http://127.0.0.1:8001\" . workspace_id ( str ) : Unique identifier for the experience workspace. Different workspaces maintain isolated experience pools. Default: \"default\" . enable_summarizer ( bool ) : Whether to activate automatic experience summarization from rollout trajectories. When True , raw reasoning traces are distilled into reusable experience snippets. Default: False . enable_context_generator ( bool ) : Whether to enable experience retrieval and context injection during rollouts. Must be True for experience-guided rollouts to function. Default: False . updated_freq ( int ) : Frequency (in training steps) to refresh the experience pool. Set to 0 to disable periodic updates. Default: 0 .","title":"ReMe Service Configuration"},{"location":"guidelines/exp_manager/#initialization-utilities","text":"init_exp_before_training ( bool ) : Whether to pre-populate the experience pool before training begins. Useful for warm-start scenarios with prior knowledge. Default: False . init_exp_only ( bool ) : If True , only initializes the experience pool without starting training. Useful for pre-computing embeddings or validating summarization quality. Default: False . summary_batch_size ( int ) : Batch size for processing experience summarization requests. Larger values improve throughput but require more memory. Default: 8 .","title":"Initialization &amp; Utilities"},{"location":"guidelines/exp_manager/#quick-start-recommended-configuration","text":"","title":"\ud83e\udded Quick Start &amp; Recommended Configuration"},{"location":"guidelines/exp_manager/#step-1-set-up-reme-service","text":"Ensure you have completed the ReMe installation ; Ensure you have modified the environment configuration ; Start the ReMe service with the following command: cd path_to_reme/ReMe reme \\ config = default \\ backend = http \\ thread_pool_max_workers = 256 \\ http.host = \"127.0.0.1\" \\ http.port = 8001 \\ http.limit_concurrency = 256 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local \\ op.rerank_memory_op.params.enable_llm_rerank = false \\ flow.summary_task_memory.flow_content = \"trajectory_preprocess_op >> (success_extraction_op|failure_extraction_op|comparative_extraction_op) >> memory_validation_op >> memory_deduplication_op >> update_vector_store_op\"","title":"Step 1: Set Up ReMe Service"},{"location":"guidelines/exp_manager/#step-2-recommended-configuration","text":"Add the following configuration to your experiment settings: exp_manager : val_rollout_mode : \"woexp\" # rollout mode on dev/test set: [\"mixed\", \"all\", \"woexp\"] train_rollout_mode : \"mixed\" # rollout mode on train set: [\"mixed\", \"all\", \"woexp\"] rollout_ratio : 0.5 # ratio of adding experience within group during rollout train_sample_mode : \"alldiscard\" # sample mode after train rollout: [\"allkeep\", \"alldiscard\", \"hybrid\"] train_sample_keepratio : 0.0 # task-level ratio for keeping experience experience_template : \"\\n\\nSome Related Experience to help you to complete the task:<EXP>{}</EXP>\\n\\n\" # template for inserting experience init_exp_before_training : True # whether to init experience pool before training init_exp_only : False # whether to only init experience pool summary_batch_size : 8 # batch size for experience summarization reme : base_url : \"http://127.0.0.1:8001\" # base URL for ReMe service workspace_id : \"default\" # workspace ID for ReMe enable_summarizer : True # whether to enable experience summarizer enable_context_generator : True # whether to enable context generator retrieve_top_k : 3 # number of top experiences to retrieve updated_freq : 0 # experience pool update frequency (in steps, 0=disabled)","title":"Step 2: Recommended Configuration"},{"location":"guidelines/exp_manager/#step-3-understanding-different-usage-modes","text":"The experience pool system supports multiple usage modes to fit different experimental needs. Configure the parameters according to the table below:","title":"Step 3: Understanding Different Usage Modes"},{"location":"guidelines/exp_manager/#mode-descriptions","text":"Usage Mode Description No Pool Training without any experience pool (baseline mode) Init Only Initialize experience pool from trajectories only, without using it during training Init + Train Initialize experience pool before training and use it throughout the training process Init + Train + Update Initialize experience pool, use during training, and continuously update it with new experiences Exist + Train Use an existing experience pool during training without initialization or updates Exist + Train + Update Use an existing experience pool during training and continuously update it with new experiences","title":"Mode Descriptions"},{"location":"guidelines/exp_manager/#configuration-matrix","text":"No Pool Init Only Init + Train Init + Train + Update Exist + Train Exist + Train + Update updated_freq 0 0 0 k != 0 0 k != 0 init_exp_only False True False False False False init_exp_before_training False True True True False False enable_summarizer False True False True False True enable_context_generator False False True True True True workspace_id - New ID New ID New ID Exist ID Exist ID","title":"Configuration Matrix"},{"location":"guidelines/exp_manager/#configuration-notes","text":"updated_freq : Set to a non-zero value (e.g., 100 ) to enable periodic experience pool updates during training. 0 disables updates. workspace_id : Use a new workspace ID to create a fresh experience pool Specify an existing workspace ID to reuse a previously created pool When using vector_store.default.backend=local , the experience pool is saved at: ReMe/local_vector_store/{workspace_id}.jsonl Recommended starting point : For most use cases, we recommend starting with Init + Train mode.","title":"Configuration Notes"},{"location":"guidelines/task_manager/","text":"To evolve an agent in a data-driven manner within your own environment, the first step is to collect training data that maps the agent's abilities to your requirements. Task Manager provides the training data for AgentEvolver. It is responsible for: Exploring unknown environments and profiling potential tasks, Discovering new synthetic tasks and capturing user requirements, Curating training tasks and managing their quality and quantity, Providing a built-in synthetic reward mechanism as a fallback. In this section, we introduce Task Manager and explain how to efficiently collect appropriate training data for agent training, including the configuration of Environment Profiling, Task Derivation, and Task Curation strategies. Collect Your First Training Data To collect your training data, perform the following five steps: Integrate your environment with Environment Service (as described in the previous section). Profile the environment. Configure the strategies to be applied, including their parameters. Execute Task Manager and collect the training data. Check your data. 1. Adopt the Environment Assume the environment has already been integrated with Environment Service. If not, please refer to the previous section for setup instructions. 2. Profile the Environment Task Manager requires not only the API specifications but also conceptual knowledge of the environment. For example, in a file system, APIs define file operations, but they do not capture high-level concepts such as file types or formats. These conceptual elements must be explicitly represented. We introduce the Environment Profile to capture these concepts. An Environment Profile is a JSON file that specifies Entity , Attribute , and Operation definitions for the environment. Entity : Represents a core object in the environment. Entities are the targets of interaction and can typically be created, modified, or deleted. Attribute : Defines descriptive properties or metadata of an entity. Attributes provide contextual information but are not executable actions. Operation : Specifies the actions that can be performed on an entity. Operations represent the functional capabilities of the environment and are often aligned with API calls. Additionally, the Environment Profile defines task preferences , which control the style and scope of the generated tasks. A basic Environment Profile example is shown below: { \"name\" : \"Alice\" , \"background\" : \"A general user working with a file system.\" , \"entities\" : [ { \"name\" : \"file\" , \"description\" : \"A file in a file system.\" , \"attrs\" : { \"name\" : \"The name of the file.\" , \"size\" : \"The size of the file in bytes.\" , \"type\" : \"The type of the file, e.g. text, image, video, etc.\" , \"parent\" : \"The parent directory of the file.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new file.\" }, { \"name\" : \"delete\" , \"description\" : \"Delete a file.\" }, { \"name\" : \"read\" , \"description\" : \"Read a file.\" }, { \"name\" : \"write\" , \"description\" : \"Write to a file.\" } ] }, { \"name\" : \"directory\" , \"description\" : \"A directory in a file system.\" , \"attrs\" : { \"name\" : \"The name of the directory.\" , \"parent\" : \"The parent directory of the directory.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new directory.\" }, { \"name\" : \"delete\" , \"description\" : \"Delete a directory.\" }, { \"name\" : \"list\" , \"description\" : \"List the contents of a directory.\" } ] } ], \"task_preference\" : { \"num_entities\" : 2 , \"num_opts\" : 3 , \"relation_difficulty\" : 3 } } In this profile, entities file and directory are defined with attributes ( name , size , type , parent ) and operations ( create , delete , read , write , list ). Based on these definitions, Task Manager gains a structured understanding of the environment to support task derivation and curation. To create your own Environment Profile, copy the template environment_profile_template.json to environment_profile.json and fill in the details. Using an LLM to assist in drafting the profile can reduce manual effort. 3. Configure the Strategies Transforming profiles into synthetic tasks involves two stages: task derivation and task curation . Task derivation is the process of generating candidate tasks from the profile. During derivation, exploration and summarization are performed under the guidance of a chosen strategy. Strategies determine how the environment is traversed and how structured tasks are extracted from exploration trajectories. Task curation ensures task quality and diversity. Filters are applied to discard infeasible, redundant, or irrelevant tasks. Mixture strategies combine tasks from multiple sources and control properties such as difficulty distribution. By default, Task Manager provides: RandomWalk Strategy for task derivation, DeduplicationFilter , FeasibilityFilter , and UnifiedMixtureStrategy for task curation. These can be configured in the YAML configuration file. After configuring strategies, the whole configuration file looks like this: task_manager : # tasks will be explored once if set. use it if you want to keep the same explorations. train_data_path : tasks_explored.train.json val_data_path : tasks_explored.val.json # model used to explore the environment llm_client : qwen-plus # repetation of exploration n : 0 # env profiles used in exploration env_profile : cookbook/env_profiles/appworld.json # batch size of dynamic synthetic data bs : ${data.train_batch_size} # use the same batch size as train_batch_size # number of threads for exploration num_explore_threads : 16 # mixture strategy mixture : # whether to use original tasks provided by the environment use_original_tasks : True synthetic_data_ratio : 0.0 # whether to shuffle tasks *after* mixture shuffle : True # the grader used to evaluate tasks grader : # grader used to evaluate tasks: env, llm original_grader : env synthetic_grader : llm # strategy used to explore the environment strategy : random strategy_args : max_explore_step : 30 max_llm_retries : 6 env_url : ${env_service.env_url} # refer to env_service.env_url exploration_llm_temperature : 1.0 exploration_llm_top_p : 1.0 exploration_llm_top_k : 100 4. Start Task Synthesis Once configuration is complete, task synthesis can be initiated. Start the Environment Service. Start Task Manager. Standalone Mode Task Manager can be executed in standalone mode for simple task synthesis. Example command: $ python -m agentevolver.module.task_manager The synthesis progress will be displayed. When the process completes, the path to the generated tasks will be printed. Integrated Mode In most workflows, Task Manager is integrated with AgentEvolver. Launching AgentEvolver automatically starts the training and task synthesis pipeline. Standalone vs Integrated Task Manager can be run independently for rapid prototyping or small-scale data generation. It is recommended to tune strategies in standalone mode, and then use integrated mode in production, where additional features are available within AgentEvolver. 5. Check the Data The generated synthetic tasks are stored in the path specified in the YAML configuration file. task_manager : # where to save the generated tasks train_data_path : tasks_explored.train.json # tasks will be explored once if set. use it if you want to keep the same explorations. val_data_path : tasks_explored.val.json # tasks will be explored once if set. use it if you want to keep the same explorations. Dynamic vs Static train_data_path and val_data_path set, tasks will be explored once and saved to the specified path. If no path is set for integrated mode, Task Manager will generate tasks dynamically during training. All synthetic tasks will be discarded after training. Inspect the generated data to ensure it aligns with your training requirements. Workflow of Task Manager In data-driven model optimization, agent training is formulated as trajectory tuning over environment-specific tasks. Consequently, the quality of training data directly determines the resulting agent capabilities. However, in real environments, acquiring and controlling the quality of training tasks is inherently difficult. Task Manager addresses this challenge by providing a dynamic and general-purpose workflow for environment exploration, task generation, and quality control. From the figure, the workflow consists of three major steps and corresponding components: Environment Exploration Environment Service \u2013 Provides interface for the environment. Environment Profile \u2013 Describes the environment. Task Derivation : Strategy \u2013 Control the exploration and summarization process. Task Curation : Filter \u2013 Control the quality of tasks. Mixture Strategy \u2013 Control the distribution of tasks. And Judge : Provide rewards for training. The following sections describe each component of Task Manager in detail, including extension points for customization. Environment Profiling An Environment Profile describes the concepts of an environment using entities , attributes , and operations . Similar to object-oriented programming and database schemas, these components are considered fundamental. Entity : Represents an object in the environment. Attributes : Define the properties of the entity. Operations : Specify the actions that can be applied to the entity. For example: Entity : File Attributes - name : The name of the file . - size : The size of the file in bytes . - type : The type of the file , e . g . text , image , video , etc . - permission : The permission of the file . Operations - create : Create a new file . - delete : Delete a file . - read : Read a file . - write : Write to a file . - chmod : Change the permission of a file . The granularity of a profile is flexible. With the assistance of LLMs, profiles can be constructed at multiple levels, ranging from a single generic entity to highly specialized entities. The choice of granularity is a trade-off between manual specification and the capability of the LLM to generalize. Task Manager leverages the Environment Profile to recognize concepts, explore relationships between entities, and synthesize meaningful tasks. Operations are combined to form candidate solutions reflecting real-world problem-solving. Users may optionally specify a User Preference in addition to the Environment Profile. Preferences define expectations for the agent's capabilities, such as desired task difficulty or task categories. Write a Profile Profiles can be specified in JSON (recommended) or in Python . Top-level structure: { \"name\" : s tr i n g , \"background\" : s tr i n g , \"entities\" : [ ... ], \"task_preference\" : { \"num_entities\" : i nte ger , \"num_opts\" : i nte ger , \"relation_difficulty\" : i nte ger } } Example entity definition: { \"name\" : \"file\" , \"description\" : \"A file in a file system.\" , \"attrs\" : { \"name\" : \"The name of the file.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new file.\" } ] } A minimal working example: { \"name\" : \"Alice\" , \"background\" : \"A general user working with a file system.\" , \"entities\" : [ { \"name\" : \"file\" , \"description\" : \"A file in a file system.\" , \"attrs\" : { \"name\" : \"The name of the file.\" , \"size\" : \"The size of the file in bytes.\" , \"type\" : \"The type of the file (e.g., text, image, video).\" , \"parent\" : \"The parent directory of the file.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new file.\" }, { \"name\" : \"delete\" , \"description\" : \"Delete a file.\" }, { \"name\" : \"read\" , \"description\" : \"Read a file.\" }, { \"name\" : \"write\" , \"description\" : \"Write to a file.\" } ] }, { \"name\" : \"directory\" , \"description\" : \"A directory in a file system.\" , \"attrs\" : { \"name\" : \"The name of the directory.\" , \"parent\" : \"The parent directory of the directory.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new directory.\" }, { \"name\" : \"delete\" , \"description\" : \"Delete a directory.\" }, { \"name\" : \"list\" , \"description\" : \"List the contents of a directory.\" } ] } ], \"task_preference\" : { \"num_entities\" : 2 , \"num_opts\" : 3 , \"relation_difficulty\" : 3 } } For examples in real environments, please refer to cookbook in the root directory. If Python is preferred, an example is from agentevolver.module.task_manager.env_profiles import EnvEntity , EnvEntityOpt , TaskPreference , EnvProfile spotify = EnvEntity ( name = \"Spotify\" , description = \"A music streaming service with song, album, and playlist management.\" , attrs = { \"Song Library\" : \"Songs saved by the user.\" , \"Album Library\" : \"Albums saved by the user.\" , \"Playlists\" : \"User-created or followed playlists.\" }, opts = [ EnvEntityOpt ( \"play_song\" , \"Play a song, album, or playlist.\" ), EnvEntityOpt ( \"like_song\" , \"Like songs or albums.\" ), EnvEntityOpt ( \"unfollow_artist\" , \"Unfollow an artist.\" ), EnvEntityOpt ( \"follow_artist\" , \"Follow an artist.\" ), EnvEntityOpt ( \"create_playlist\" , \"Create a new playlist.\" ), EnvEntityOpt ( \"remove_song\" , \"Remove songs from library or playlist.\" ), EnvEntityOpt ( \"export_library\" , \"Export song/album/playlist data.\" ) ] ) gmail = EnvEntity ( name = \"Gmail\" , description = \"An email service for sending, receiving, labeling, and managing emails.\" , attrs = { \"Inbox\" : \"List of received email threads.\" , \"Outbox\" : \"List of sent email threads.\" , \"Labels\" : \"Custom labels to organize emails.\" }, opts = [ EnvEntityOpt ( \"send_email\" , \"Send an email.\" ), EnvEntityOpt ( \"forward_email\" , \"Forward an email.\" ), EnvEntityOpt ( \"reply_email\" , \"Reply to an email.\" ), EnvEntityOpt ( \"delete_email\" , \"Delete emails.\" ), EnvEntityOpt ( \"label_email\" , \"Label emails.\" ), EnvEntityOpt ( \"star_email\" , \"Star or unstar email threads.\" ) ] ) env_profile = EnvProfile ( name = \"Bob\" , background = \"A general computer user.\" , task = TaskPreference ( num_entities = 2 , num_opts = 3 , relation_difficulty = 3 , ) ) env_profile . reg_entities ([ spotify , gmail ]) More examples can be found in agentevolver/module/task_manager/prelude_profiles . Task Derivation Task Derivation is the initial stage of synthetic task generation. It transforms the Environment Profile into preliminary task drafts by applying exploration and synthesis strategies. The primary objectives are: Exploration \u2013 Cover the environment systematically or stochastically. Summarization \u2013 Convert exploration trajectories into concise, structured candidate tasks. Available strategies: RandomWalk Strategy \u2013 Random exploration producing a diverse task set. Additional strategies will be introduced in future versions. RandomWalk Strategy The RandomWalk Strategy is simple yet effective. It explores the environment by sampling entities and operations at random, generating diverse trajectories that can be summarized into tasks. Parameters: task_manager : strategy : random strategy_args : max_explore_step : 30 max_llm_retries : 6 env_url : ${env_service.env_url} # refer to env_service.env_url exploration_llm_temperature : 1.0 exploration_llm_top_p : 1.0 exploration_llm_top_k : 100 Task Curation Task Curation ensures the quality and diversity of tasks generated during derivation by applying filters and mixture strategies. Filters DeduplicationFilter : Removes redundant or near-duplicate tasks. FeasibilityFilter : Removes tasks that cannot be executed in the environment. Mixture Strategies UnifiedMixtureStrategy : Combines tasks from multiple sources to maintain balance. Goals of curation: Quality assurance \u2013 Ensure valid, feasible, and logically sound tasks. Diversity preservation \u2013 Avoid bias toward a single task type. Dynamic control \u2013 Adjust task selection according to agent progress. DeduplicationFilter Removes duplicate or highly similar tasks to improve data diversity. Enabled by default. FeasibilityFilter Filters out tasks that cannot be completed given the environment constraints. Synthetic Reward Task Manager provides a built-in synthetic reward as a fallback, enabling training without requiring user-defined reward functions. Key properties: Generality \u2013 Applicable across diverse environments. Zero-configuration \u2013 Works out of the box. Extensibility \u2013 Can be replaced or extended with custom reward functions. Typical reward components: Relevance check \u2013 Whether the trajectory matches the task. Success check \u2013 Whether the task is successfully completed. Efficiency check \u2013 Whether the task is solved within reasonable steps. Although the built-in synthetic reward provides a functional fallback, we strongly recommend implementing domain-specific reward functions to achieve optimal agent performance. Configuration example: task_manager : grader : original_grader : env synthetic_grader : llm Extend Task Manager Task Manager is designed as a modular and extensible framework , adaptable to different training scenarios. To extend Task Manager, users can implement and replace the components used in three stages in the pipeline, including Strategy , Filter , Mixture Strategy , and Judge .","title":"Task Manager"},{"location":"guidelines/task_manager/#collect-your-first-training-data","text":"To collect your training data, perform the following five steps: Integrate your environment with Environment Service (as described in the previous section). Profile the environment. Configure the strategies to be applied, including their parameters. Execute Task Manager and collect the training data. Check your data.","title":"Collect Your First Training Data"},{"location":"guidelines/task_manager/#1-adopt-the-environment","text":"Assume the environment has already been integrated with Environment Service. If not, please refer to the previous section for setup instructions.","title":"1. Adopt the Environment"},{"location":"guidelines/task_manager/#2-profile-the-environment","text":"Task Manager requires not only the API specifications but also conceptual knowledge of the environment. For example, in a file system, APIs define file operations, but they do not capture high-level concepts such as file types or formats. These conceptual elements must be explicitly represented. We introduce the Environment Profile to capture these concepts. An Environment Profile is a JSON file that specifies Entity , Attribute , and Operation definitions for the environment. Entity : Represents a core object in the environment. Entities are the targets of interaction and can typically be created, modified, or deleted. Attribute : Defines descriptive properties or metadata of an entity. Attributes provide contextual information but are not executable actions. Operation : Specifies the actions that can be performed on an entity. Operations represent the functional capabilities of the environment and are often aligned with API calls. Additionally, the Environment Profile defines task preferences , which control the style and scope of the generated tasks. A basic Environment Profile example is shown below: { \"name\" : \"Alice\" , \"background\" : \"A general user working with a file system.\" , \"entities\" : [ { \"name\" : \"file\" , \"description\" : \"A file in a file system.\" , \"attrs\" : { \"name\" : \"The name of the file.\" , \"size\" : \"The size of the file in bytes.\" , \"type\" : \"The type of the file, e.g. text, image, video, etc.\" , \"parent\" : \"The parent directory of the file.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new file.\" }, { \"name\" : \"delete\" , \"description\" : \"Delete a file.\" }, { \"name\" : \"read\" , \"description\" : \"Read a file.\" }, { \"name\" : \"write\" , \"description\" : \"Write to a file.\" } ] }, { \"name\" : \"directory\" , \"description\" : \"A directory in a file system.\" , \"attrs\" : { \"name\" : \"The name of the directory.\" , \"parent\" : \"The parent directory of the directory.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new directory.\" }, { \"name\" : \"delete\" , \"description\" : \"Delete a directory.\" }, { \"name\" : \"list\" , \"description\" : \"List the contents of a directory.\" } ] } ], \"task_preference\" : { \"num_entities\" : 2 , \"num_opts\" : 3 , \"relation_difficulty\" : 3 } } In this profile, entities file and directory are defined with attributes ( name , size , type , parent ) and operations ( create , delete , read , write , list ). Based on these definitions, Task Manager gains a structured understanding of the environment to support task derivation and curation. To create your own Environment Profile, copy the template environment_profile_template.json to environment_profile.json and fill in the details. Using an LLM to assist in drafting the profile can reduce manual effort.","title":"2. Profile the Environment"},{"location":"guidelines/task_manager/#3-configure-the-strategies","text":"Transforming profiles into synthetic tasks involves two stages: task derivation and task curation . Task derivation is the process of generating candidate tasks from the profile. During derivation, exploration and summarization are performed under the guidance of a chosen strategy. Strategies determine how the environment is traversed and how structured tasks are extracted from exploration trajectories. Task curation ensures task quality and diversity. Filters are applied to discard infeasible, redundant, or irrelevant tasks. Mixture strategies combine tasks from multiple sources and control properties such as difficulty distribution. By default, Task Manager provides: RandomWalk Strategy for task derivation, DeduplicationFilter , FeasibilityFilter , and UnifiedMixtureStrategy for task curation. These can be configured in the YAML configuration file. After configuring strategies, the whole configuration file looks like this: task_manager : # tasks will be explored once if set. use it if you want to keep the same explorations. train_data_path : tasks_explored.train.json val_data_path : tasks_explored.val.json # model used to explore the environment llm_client : qwen-plus # repetation of exploration n : 0 # env profiles used in exploration env_profile : cookbook/env_profiles/appworld.json # batch size of dynamic synthetic data bs : ${data.train_batch_size} # use the same batch size as train_batch_size # number of threads for exploration num_explore_threads : 16 # mixture strategy mixture : # whether to use original tasks provided by the environment use_original_tasks : True synthetic_data_ratio : 0.0 # whether to shuffle tasks *after* mixture shuffle : True # the grader used to evaluate tasks grader : # grader used to evaluate tasks: env, llm original_grader : env synthetic_grader : llm # strategy used to explore the environment strategy : random strategy_args : max_explore_step : 30 max_llm_retries : 6 env_url : ${env_service.env_url} # refer to env_service.env_url exploration_llm_temperature : 1.0 exploration_llm_top_p : 1.0 exploration_llm_top_k : 100","title":"3. Configure the Strategies"},{"location":"guidelines/task_manager/#4-start-task-synthesis","text":"Once configuration is complete, task synthesis can be initiated. Start the Environment Service. Start Task Manager.","title":"4. Start Task Synthesis"},{"location":"guidelines/task_manager/#standalone-mode","text":"Task Manager can be executed in standalone mode for simple task synthesis. Example command: $ python -m agentevolver.module.task_manager The synthesis progress will be displayed. When the process completes, the path to the generated tasks will be printed.","title":"Standalone Mode"},{"location":"guidelines/task_manager/#integrated-mode","text":"In most workflows, Task Manager is integrated with AgentEvolver. Launching AgentEvolver automatically starts the training and task synthesis pipeline. Standalone vs Integrated Task Manager can be run independently for rapid prototyping or small-scale data generation. It is recommended to tune strategies in standalone mode, and then use integrated mode in production, where additional features are available within AgentEvolver.","title":"Integrated Mode"},{"location":"guidelines/task_manager/#5-check-the-data","text":"The generated synthetic tasks are stored in the path specified in the YAML configuration file. task_manager : # where to save the generated tasks train_data_path : tasks_explored.train.json # tasks will be explored once if set. use it if you want to keep the same explorations. val_data_path : tasks_explored.val.json # tasks will be explored once if set. use it if you want to keep the same explorations. Dynamic vs Static train_data_path and val_data_path set, tasks will be explored once and saved to the specified path. If no path is set for integrated mode, Task Manager will generate tasks dynamically during training. All synthetic tasks will be discarded after training. Inspect the generated data to ensure it aligns with your training requirements.","title":"5. Check the Data"},{"location":"guidelines/task_manager/#workflow-of-task-manager","text":"In data-driven model optimization, agent training is formulated as trajectory tuning over environment-specific tasks. Consequently, the quality of training data directly determines the resulting agent capabilities. However, in real environments, acquiring and controlling the quality of training tasks is inherently difficult. Task Manager addresses this challenge by providing a dynamic and general-purpose workflow for environment exploration, task generation, and quality control. From the figure, the workflow consists of three major steps and corresponding components: Environment Exploration Environment Service \u2013 Provides interface for the environment. Environment Profile \u2013 Describes the environment. Task Derivation : Strategy \u2013 Control the exploration and summarization process. Task Curation : Filter \u2013 Control the quality of tasks. Mixture Strategy \u2013 Control the distribution of tasks. And Judge : Provide rewards for training. The following sections describe each component of Task Manager in detail, including extension points for customization.","title":"Workflow of Task Manager"},{"location":"guidelines/task_manager/#environment-profiling","text":"An Environment Profile describes the concepts of an environment using entities , attributes , and operations . Similar to object-oriented programming and database schemas, these components are considered fundamental. Entity : Represents an object in the environment. Attributes : Define the properties of the entity. Operations : Specify the actions that can be applied to the entity. For example: Entity : File Attributes - name : The name of the file . - size : The size of the file in bytes . - type : The type of the file , e . g . text , image , video , etc . - permission : The permission of the file . Operations - create : Create a new file . - delete : Delete a file . - read : Read a file . - write : Write to a file . - chmod : Change the permission of a file . The granularity of a profile is flexible. With the assistance of LLMs, profiles can be constructed at multiple levels, ranging from a single generic entity to highly specialized entities. The choice of granularity is a trade-off between manual specification and the capability of the LLM to generalize. Task Manager leverages the Environment Profile to recognize concepts, explore relationships between entities, and synthesize meaningful tasks. Operations are combined to form candidate solutions reflecting real-world problem-solving. Users may optionally specify a User Preference in addition to the Environment Profile. Preferences define expectations for the agent's capabilities, such as desired task difficulty or task categories.","title":"Environment Profiling"},{"location":"guidelines/task_manager/#write-a-profile","text":"Profiles can be specified in JSON (recommended) or in Python . Top-level structure: { \"name\" : s tr i n g , \"background\" : s tr i n g , \"entities\" : [ ... ], \"task_preference\" : { \"num_entities\" : i nte ger , \"num_opts\" : i nte ger , \"relation_difficulty\" : i nte ger } } Example entity definition: { \"name\" : \"file\" , \"description\" : \"A file in a file system.\" , \"attrs\" : { \"name\" : \"The name of the file.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new file.\" } ] } A minimal working example: { \"name\" : \"Alice\" , \"background\" : \"A general user working with a file system.\" , \"entities\" : [ { \"name\" : \"file\" , \"description\" : \"A file in a file system.\" , \"attrs\" : { \"name\" : \"The name of the file.\" , \"size\" : \"The size of the file in bytes.\" , \"type\" : \"The type of the file (e.g., text, image, video).\" , \"parent\" : \"The parent directory of the file.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new file.\" }, { \"name\" : \"delete\" , \"description\" : \"Delete a file.\" }, { \"name\" : \"read\" , \"description\" : \"Read a file.\" }, { \"name\" : \"write\" , \"description\" : \"Write to a file.\" } ] }, { \"name\" : \"directory\" , \"description\" : \"A directory in a file system.\" , \"attrs\" : { \"name\" : \"The name of the directory.\" , \"parent\" : \"The parent directory of the directory.\" }, \"opts\" : [ { \"name\" : \"create\" , \"description\" : \"Create a new directory.\" }, { \"name\" : \"delete\" , \"description\" : \"Delete a directory.\" }, { \"name\" : \"list\" , \"description\" : \"List the contents of a directory.\" } ] } ], \"task_preference\" : { \"num_entities\" : 2 , \"num_opts\" : 3 , \"relation_difficulty\" : 3 } } For examples in real environments, please refer to cookbook in the root directory. If Python is preferred, an example is from agentevolver.module.task_manager.env_profiles import EnvEntity , EnvEntityOpt , TaskPreference , EnvProfile spotify = EnvEntity ( name = \"Spotify\" , description = \"A music streaming service with song, album, and playlist management.\" , attrs = { \"Song Library\" : \"Songs saved by the user.\" , \"Album Library\" : \"Albums saved by the user.\" , \"Playlists\" : \"User-created or followed playlists.\" }, opts = [ EnvEntityOpt ( \"play_song\" , \"Play a song, album, or playlist.\" ), EnvEntityOpt ( \"like_song\" , \"Like songs or albums.\" ), EnvEntityOpt ( \"unfollow_artist\" , \"Unfollow an artist.\" ), EnvEntityOpt ( \"follow_artist\" , \"Follow an artist.\" ), EnvEntityOpt ( \"create_playlist\" , \"Create a new playlist.\" ), EnvEntityOpt ( \"remove_song\" , \"Remove songs from library or playlist.\" ), EnvEntityOpt ( \"export_library\" , \"Export song/album/playlist data.\" ) ] ) gmail = EnvEntity ( name = \"Gmail\" , description = \"An email service for sending, receiving, labeling, and managing emails.\" , attrs = { \"Inbox\" : \"List of received email threads.\" , \"Outbox\" : \"List of sent email threads.\" , \"Labels\" : \"Custom labels to organize emails.\" }, opts = [ EnvEntityOpt ( \"send_email\" , \"Send an email.\" ), EnvEntityOpt ( \"forward_email\" , \"Forward an email.\" ), EnvEntityOpt ( \"reply_email\" , \"Reply to an email.\" ), EnvEntityOpt ( \"delete_email\" , \"Delete emails.\" ), EnvEntityOpt ( \"label_email\" , \"Label emails.\" ), EnvEntityOpt ( \"star_email\" , \"Star or unstar email threads.\" ) ] ) env_profile = EnvProfile ( name = \"Bob\" , background = \"A general computer user.\" , task = TaskPreference ( num_entities = 2 , num_opts = 3 , relation_difficulty = 3 , ) ) env_profile . reg_entities ([ spotify , gmail ]) More examples can be found in agentevolver/module/task_manager/prelude_profiles .","title":"Write a Profile"},{"location":"guidelines/task_manager/#task-derivation","text":"Task Derivation is the initial stage of synthetic task generation. It transforms the Environment Profile into preliminary task drafts by applying exploration and synthesis strategies. The primary objectives are: Exploration \u2013 Cover the environment systematically or stochastically. Summarization \u2013 Convert exploration trajectories into concise, structured candidate tasks. Available strategies: RandomWalk Strategy \u2013 Random exploration producing a diverse task set. Additional strategies will be introduced in future versions.","title":"Task Derivation"},{"location":"guidelines/task_manager/#randomwalk-strategy","text":"The RandomWalk Strategy is simple yet effective. It explores the environment by sampling entities and operations at random, generating diverse trajectories that can be summarized into tasks. Parameters: task_manager : strategy : random strategy_args : max_explore_step : 30 max_llm_retries : 6 env_url : ${env_service.env_url} # refer to env_service.env_url exploration_llm_temperature : 1.0 exploration_llm_top_p : 1.0 exploration_llm_top_k : 100","title":"RandomWalk Strategy"},{"location":"guidelines/task_manager/#task-curation","text":"Task Curation ensures the quality and diversity of tasks generated during derivation by applying filters and mixture strategies. Filters DeduplicationFilter : Removes redundant or near-duplicate tasks. FeasibilityFilter : Removes tasks that cannot be executed in the environment. Mixture Strategies UnifiedMixtureStrategy : Combines tasks from multiple sources to maintain balance. Goals of curation: Quality assurance \u2013 Ensure valid, feasible, and logically sound tasks. Diversity preservation \u2013 Avoid bias toward a single task type. Dynamic control \u2013 Adjust task selection according to agent progress.","title":"Task Curation"},{"location":"guidelines/task_manager/#deduplicationfilter","text":"Removes duplicate or highly similar tasks to improve data diversity. Enabled by default.","title":"DeduplicationFilter"},{"location":"guidelines/task_manager/#feasibilityfilter","text":"Filters out tasks that cannot be completed given the environment constraints.","title":"FeasibilityFilter"},{"location":"guidelines/task_manager/#synthetic-reward","text":"Task Manager provides a built-in synthetic reward as a fallback, enabling training without requiring user-defined reward functions. Key properties: Generality \u2013 Applicable across diverse environments. Zero-configuration \u2013 Works out of the box. Extensibility \u2013 Can be replaced or extended with custom reward functions. Typical reward components: Relevance check \u2013 Whether the trajectory matches the task. Success check \u2013 Whether the task is successfully completed. Efficiency check \u2013 Whether the task is solved within reasonable steps. Although the built-in synthetic reward provides a functional fallback, we strongly recommend implementing domain-specific reward functions to achieve optimal agent performance. Configuration example: task_manager : grader : original_grader : env synthetic_grader : llm","title":"Synthetic Reward"},{"location":"guidelines/task_manager/#extend-task-manager","text":"Task Manager is designed as a modular and extensible framework , adaptable to different training scenarios. To extend Task Manager, users can implement and replace the components used in three stages in the pipeline, including Strategy , Filter , Mixture Strategy , and Judge .","title":"Extend Task Manager"},{"location":"tutorial/configuration/","text":"Overview AgentEvolver uses a hierarchical configuration system based on Hydra , which allows for flexible and modular configuration management. The configuration consists of multiple layers that inherit and override settings, enabling developers to customize behavior for different use cases. Configuration Hierarchy The configuration implements a three-layered inheritance model: Base Layer : ppo_trainer.yaml provides system defaults Framework Layer : agentevolver.yaml customizes for the framework Application Layer : Example configuration files ( basic.yaml / overall.yaml ) or scripts ( run_basic.sh / run_overall.sh ) provide final customizations 1. Base Configuration: external/config_fallback/ppo_trainer.yaml This file serves as the foundation of the configuration system, containing default values for all core parameters needed for training derived from veRL. For detailed documentation, refer to the veRL . 2. AgentEvolver Configuration: config/agentevolver.yaml This file extends and customizes the base configuration for AgentEvolver features. Overrides algorithm parameters for training algorithm Configures self-questioning, -navigating, and -attributing functionality Defines experiment projects and names 3. Application Configuration examples/basic.yaml : Minimal example for getting started examples/overall.yaml : Comprehensive setup for all features config/script_config.yaml : Defines interface for script. In most of the cases, you do not need to modify this file. These files provide final customizations for specific use cases. Customizing Configurations Method 1: New YAML Files Create a new configuration file (e.g., examples/my_config.yaml ): hydra : searchpath : - file://external/config_fallback - file://config defaults : - ppo_trainer - agentevolver - _self_ # Custom overrides trainer : experiment_name : my_experiment total_epochs : 50 data : train_batch_size : 64 Launch with: python launcher.py --conf examples/my_config.yaml Method 2: Command-Line Overrides Modify or create a script similar to examples/run_basic.sh : python3 -m agentevolver.main_ppo \\ --config-path = \" $CONFIG_PATH \" \\ --config-name = 'script_config' \\ trainer.experiment_name = my_experiment \\ data.train_batch_size = 64","title":"Configuration"},{"location":"tutorial/configuration/#overview","text":"AgentEvolver uses a hierarchical configuration system based on Hydra , which allows for flexible and modular configuration management. The configuration consists of multiple layers that inherit and override settings, enabling developers to customize behavior for different use cases.","title":"Overview"},{"location":"tutorial/configuration/#configuration-hierarchy","text":"The configuration implements a three-layered inheritance model: Base Layer : ppo_trainer.yaml provides system defaults Framework Layer : agentevolver.yaml customizes for the framework Application Layer : Example configuration files ( basic.yaml / overall.yaml ) or scripts ( run_basic.sh / run_overall.sh ) provide final customizations","title":"Configuration Hierarchy"},{"location":"tutorial/configuration/#1-base-configuration-externalconfig_fallbackppo_traineryaml","text":"This file serves as the foundation of the configuration system, containing default values for all core parameters needed for training derived from veRL. For detailed documentation, refer to the veRL .","title":"1. Base Configuration: external/config_fallback/ppo_trainer.yaml"},{"location":"tutorial/configuration/#2-agentevolver-configuration-configagentevolveryaml","text":"This file extends and customizes the base configuration for AgentEvolver features. Overrides algorithm parameters for training algorithm Configures self-questioning, -navigating, and -attributing functionality Defines experiment projects and names","title":"2. AgentEvolver Configuration: config/agentevolver.yaml"},{"location":"tutorial/configuration/#3-application-configuration","text":"examples/basic.yaml : Minimal example for getting started examples/overall.yaml : Comprehensive setup for all features config/script_config.yaml : Defines interface for script. In most of the cases, you do not need to modify this file. These files provide final customizations for specific use cases.","title":"3. Application Configuration"},{"location":"tutorial/configuration/#customizing-configurations","text":"","title":"Customizing Configurations"},{"location":"tutorial/configuration/#method-1-new-yaml-files","text":"Create a new configuration file (e.g., examples/my_config.yaml ): hydra : searchpath : - file://external/config_fallback - file://config defaults : - ppo_trainer - agentevolver - _self_ # Custom overrides trainer : experiment_name : my_experiment total_epochs : 50 data : train_batch_size : 64 Launch with: python launcher.py --conf examples/my_config.yaml","title":"Method 1: New YAML Files"},{"location":"tutorial/configuration/#method-2-command-line-overrides","text":"Modify or create a script similar to examples/run_basic.sh : python3 -m agentevolver.main_ppo \\ --config-path = \" $CONFIG_PATH \" \\ --config-name = 'script_config' \\ trainer.experiment_name = my_experiment \\ data.train_batch_size = 64","title":"Method 2: Command-Line Overrides"},{"location":"tutorial/install/","text":"Step 1. Basic Dependency Installation Make sure you have conda and cuda toolkit installed. Then, set up the training environment by running the script bash install.sh Step 2. Setup Env-Service (Appworld as example) The script below sets up an environment for appworld. cd env_service/environments/appworld && bash setup.sh For other environment setup, refer to docs/guidelines/env_service.md \ud83d\udcc4 Step 3. Setup ReMe (Optional) Set up the ReMe for experience management by running the script: bash external/reme/install_reme.sh For more detailed installation, please refer to ReMe .","title":"Installation"},{"location":"tutorial/install/#step-1-basic-dependency-installation","text":"Make sure you have conda and cuda toolkit installed. Then, set up the training environment by running the script bash install.sh","title":"Step 1. Basic Dependency Installation"},{"location":"tutorial/install/#step-2-setup-env-service-appworld-as-example","text":"The script below sets up an environment for appworld. cd env_service/environments/appworld && bash setup.sh For other environment setup, refer to docs/guidelines/env_service.md \ud83d\udcc4","title":"Step 2. Setup Env-Service (Appworld as example)"},{"location":"tutorial/install/#step-3-setup-reme-optional","text":"Set up the ReMe for experience management by running the script: bash external/reme/install_reme.sh For more detailed installation, please refer to ReMe .","title":"Step 3. Setup ReMe (Optional)"},{"location":"tutorial/quick_start/","text":"This guide provides two distinct paths for training an agent: Basic GRPO Training : A standard method to get started quickly. AgentEvolver Training : An advanced method that supports a self-evolving agent training. Prerequisites: One-Time Global Setup Before you begin, run these commands in your terminal to configure your environment. You only need to do this once. Initialize Conda source <YOUR_CONDA_PATH>/etc/profile.d/conda.sh Configure API Endpoints export DASHSCOPE_API_KEY = \"<YOUR_API_KEY>\" export HF_ENDPOINT = https://hf-mirror.com \ud83d\udca1 Tip: Add the export commands to your ~/.bashrc or ~/.zshrc file to set them automatically in new terminal sessions. Part A: Basic GRPO Training Step 1: Setup Env-Service (AppWorld for example) This launches the simulation environment (e.g., AppWorld) where the agent will operate. This service will run in the background. You'll need a new terminal for the next step. conda activate appworld bash env_service/launch_script/appworld.sh Step 2: Start Basic GRPO Training This command starts the training process using the GRPO method. conda activate agentevolver bash examples/run_basic.sh Part B: AgentEvolver Training Step 1: Setup Env-Service (AppWorld for example) Just like in basic training, this launches the agent's simulation environment. This service will run in the background. You'll need a new terminal for the next step. conda activate appworld bash env_service/launch_script/appworld.sh Step 2: Setup ReMe-Service This service gives the agent long-term memory and the ability to reflect on past actions. This service will listen for requests on http://127.0.0.1:8001. Keep this terminal open. Configure API Endpoints: export FLOW_EMBEDDING_API_KEY = \"<YOUR_API_KEY>\" export FLOW_EMBEDDING_BASE_URL = https://dashscope.aliyuncs.com/compatible-mode/v1 export FLOW_LLM_API_KEY = \"<YOUR_API_KEY>\" export FLOW_LLM_BASE_URL = https://dashscope.aliyuncs.com/compatible-mode/v1 conda activate reme cd external/reme reme \\ config = default \\ backend = http \\ thread_pool_max_workers = 256 \\ http.host = \"127.0.0.1\" \\ http.port = 8001 \\ http.limit_concurrency = 256 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local \\ op.rerank_memory_op.params.enable_llm_rerank = false Step 3: Start AgentEvolver Training With the environment and ReMe services running, start the AgentEvolver training. conda activate agentevolver bash examples/run_overall.sh Part C: Multi-Node Training Please read Part A and Part B first. To run AgentEvolver and train agents on multiple nodes, we need some extra setup. Step 0. Prepare the Training Follow the instructions in Configuration to prepare the training environment on all nodes. Step 1. Start the Ray cluster In Part A and Part B, we leave the Ray cluster to be started by AgentEvolver. To enable multi-node training, we need to start the Ray cluster manually. Start Ray on all nodes: # all ray node must be in the same conda environment conda activate agentevolver # node 1 as head ray start --head # other nodes as followers ray start --address = '<head addr>' Step 2. Start Env-Service (AppWorld for example) Start envservice on one node: conda activate appworld cd env_service bash launch_script/appworld.sh Once you see the successful startup logs, you're good to go. The address of this node will be used in the next step. If ReMe-Service is needed, start it on one node. Step 3. Start AgentEvolver Create a launch script based on run_basic.sh or run_overall.sh : Update env_url to the address of the envservice node Adjust other training parameters as needed Then start training from one node: conda activate agentevolver bash run_basic.sh AgentEvolver will automatically use the Ray cluster and spread the training tasks to all nodes.","title":"Quick Start"},{"location":"tutorial/quick_start/#prerequisites-one-time-global-setup","text":"Before you begin, run these commands in your terminal to configure your environment. You only need to do this once. Initialize Conda source <YOUR_CONDA_PATH>/etc/profile.d/conda.sh Configure API Endpoints export DASHSCOPE_API_KEY = \"<YOUR_API_KEY>\" export HF_ENDPOINT = https://hf-mirror.com \ud83d\udca1 Tip: Add the export commands to your ~/.bashrc or ~/.zshrc file to set them automatically in new terminal sessions.","title":"Prerequisites: One-Time Global Setup"},{"location":"tutorial/quick_start/#part-a-basic-grpo-training","text":"","title":"Part A: Basic GRPO Training"},{"location":"tutorial/quick_start/#step-1-setup-env-service-appworld-for-example","text":"This launches the simulation environment (e.g., AppWorld) where the agent will operate. This service will run in the background. You'll need a new terminal for the next step. conda activate appworld bash env_service/launch_script/appworld.sh","title":"Step 1: Setup Env-Service (AppWorld for example)"},{"location":"tutorial/quick_start/#step-2-start-basic-grpo-training","text":"This command starts the training process using the GRPO method. conda activate agentevolver bash examples/run_basic.sh","title":"Step 2: Start Basic GRPO Training"},{"location":"tutorial/quick_start/#part-b-agentevolver-training","text":"","title":"Part B: AgentEvolver Training"},{"location":"tutorial/quick_start/#step-1-setup-env-service-appworld-for-example_1","text":"Just like in basic training, this launches the agent's simulation environment. This service will run in the background. You'll need a new terminal for the next step. conda activate appworld bash env_service/launch_script/appworld.sh","title":"Step 1: Setup Env-Service (AppWorld for example)"},{"location":"tutorial/quick_start/#step-2-setup-reme-service","text":"This service gives the agent long-term memory and the ability to reflect on past actions. This service will listen for requests on http://127.0.0.1:8001. Keep this terminal open. Configure API Endpoints: export FLOW_EMBEDDING_API_KEY = \"<YOUR_API_KEY>\" export FLOW_EMBEDDING_BASE_URL = https://dashscope.aliyuncs.com/compatible-mode/v1 export FLOW_LLM_API_KEY = \"<YOUR_API_KEY>\" export FLOW_LLM_BASE_URL = https://dashscope.aliyuncs.com/compatible-mode/v1 conda activate reme cd external/reme reme \\ config = default \\ backend = http \\ thread_pool_max_workers = 256 \\ http.host = \"127.0.0.1\" \\ http.port = 8001 \\ http.limit_concurrency = 256 \\ llm.default.model_name = qwen-max-2025-01-25 \\ embedding_model.default.model_name = text-embedding-v4 \\ vector_store.default.backend = local \\ op.rerank_memory_op.params.enable_llm_rerank = false","title":"Step 2: Setup ReMe-Service"},{"location":"tutorial/quick_start/#step-3-start-agentevolver-training","text":"With the environment and ReMe services running, start the AgentEvolver training. conda activate agentevolver bash examples/run_overall.sh","title":"Step 3: Start AgentEvolver Training"},{"location":"tutorial/quick_start/#part-c-multi-node-training","text":"Please read Part A and Part B first. To run AgentEvolver and train agents on multiple nodes, we need some extra setup.","title":"Part C: Multi-Node Training"},{"location":"tutorial/quick_start/#step-0-prepare-the-training","text":"Follow the instructions in Configuration to prepare the training environment on all nodes.","title":"Step 0. Prepare the Training"},{"location":"tutorial/quick_start/#step-1-start-the-ray-cluster","text":"In Part A and Part B, we leave the Ray cluster to be started by AgentEvolver. To enable multi-node training, we need to start the Ray cluster manually. Start Ray on all nodes: # all ray node must be in the same conda environment conda activate agentevolver # node 1 as head ray start --head # other nodes as followers ray start --address = '<head addr>'","title":"Step 1. Start the Ray cluster"},{"location":"tutorial/quick_start/#step-2-start-env-service-appworld-for-example","text":"Start envservice on one node: conda activate appworld cd env_service bash launch_script/appworld.sh Once you see the successful startup logs, you're good to go. The address of this node will be used in the next step. If ReMe-Service is needed, start it on one node.","title":"Step 2. Start Env-Service (AppWorld for example)"},{"location":"tutorial/quick_start/#step-3-start-agentevolver","text":"Create a launch script based on run_basic.sh or run_overall.sh : Update env_url to the address of the envservice node Adjust other training parameters as needed Then start training from one node: conda activate agentevolver bash run_basic.sh AgentEvolver will automatically use the Ray cluster and spread the training tasks to all nodes.","title":"Step 3. Start AgentEvolver"}]}