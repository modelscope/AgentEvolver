import torch
import numpy as np

def debug_advantage_structure(batch, tokenizer=None, sample_idx=None):
    """
    è°ƒè¯•å‡½æ•°ï¼šæ£€æŸ¥advantageçš„ç»“æ„æ˜¯å¦ç¬¦åˆGRPOé¢„æœŸ
    
    Args:
        batch: DataProto batch
        tokenizer: tokenizer for decoding
        sample_idx: æŒ‡å®šè¦åˆ†æçš„æ ·æœ¬ç´¢å¼•ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ‰¾ç¬¬ä¸€ä¸ªéé›¶advantageæ ·æœ¬
    """
    print("="*50)
    print("ADVANTAGE STRUCTURE DEBUG")
    print("="*50)
    
    advantages = batch.batch["advantages"]
    response_mask = batch.batch.get("response_mask", None)
    
    # è·å–loss_maskçš„responseéƒ¨åˆ†
    loss_mask = None
    if "loss_mask" in batch.batch:
        response_length = advantages.shape[1]  # response_len
        loss_mask = batch.batch["loss_mask"][:, -response_length:]  # å–responseéƒ¨åˆ†
        print(f"Using loss_mask for analysis (multi-turn mode)")
    else:
        print(f"Using response_mask for analysis (single-turn mode)")
    
    print(f"Advantages shape: {advantages.shape}")
    print(f"Advantages dtype: {advantages.dtype}")
    print(f"Batch size: {advantages.shape[0]}")
    
    # è‡ªåŠ¨æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰éé›¶advantageçš„æ ·æœ¬
    if sample_idx is None:
        from beyondagent.module.advantage_assignment.parallel_semantic_assignment import _get_overall_advantage
        for i in range(len(advantages)):
            effective_mask = loss_mask[i] if loss_mask is not None else (response_mask[i] if response_mask is not None else None)
            adv_val = _get_overall_advantage(advantages[i], effective_mask)
            if abs(adv_val) > 1e-8:
                sample_idx = i
                print(f"ğŸ¯ Auto-selected sample {sample_idx} (first non-zero advantage: {adv_val:.6f})")
                break
        
        if sample_idx is None:
            sample_idx = 0
            print(f"âš ï¸  No non-zero advantage found, using sample 0")
    
    # æ£€æŸ¥é€‰å®šæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
    sample_adv = advantages[sample_idx]
    print(f"\nSample {sample_idx} analysis:")
    print(f"  Shape: {sample_adv.shape}")
    print(f"  Min: {sample_adv.min().item():.6f}")
    print(f"  Max: {sample_adv.max().item():.6f}")
    print(f"  Mean: {sample_adv.mean().item():.6f}")
    print(f"  Std: {sample_adv.std().item():.6f}")
    
    # æ£€æŸ¥éé›¶å€¼çš„æƒ…å†µ
    non_zero_mask = torch.abs(sample_adv) > 1e-8
    non_zero_values = sample_adv[non_zero_mask]
    
    print(f"  Non-zero count: {non_zero_mask.sum().item()}")
    if len(non_zero_values) > 0:
        print(f"  Non-zero values unique count: {torch.unique(non_zero_values).shape[0]}")
        print(f"  First non-zero value: {non_zero_values[0].item():.6f}")
        if len(non_zero_values) > 1:
            print(f"  All non-zero values same?: {torch.allclose(non_zero_values, non_zero_values[0])}")
    
    # åˆ†ææœ‰æ•ˆtokençš„advantageï¼ˆä¼˜å…ˆä½¿ç”¨loss_maskï¼‰
    effective_mask = loss_mask[sample_idx] if loss_mask is not None else (response_mask[sample_idx] if response_mask is not None else None)
    
    if effective_mask is not None:
        valid_advantages = sample_adv[effective_mask.bool()]
        mask_name = "loss_mask" if loss_mask is not None else "response_mask"
        print(f"  Valid tokens count ({mask_name}): {effective_mask.sum().item()}")
        if len(valid_advantages) > 0:
            print(f"  Valid advantages unique count: {torch.unique(valid_advantages).shape[0]}")
            print(f"  All valid advantages same?: {torch.allclose(valid_advantages, valid_advantages[0]) if len(valid_advantages) > 1 else True}")
    
    # æ£€æŸ¥ä½¿ç”¨ä¸åŒæ–¹æ³•è®¡ç®—çš„overall advantage
    print(f"\nOverall advantage calculation methods:")
    
    # æ–¹æ³•1ï¼šé”™è¯¯çš„sumæ–¹æ³•
    wrong_sum = sample_adv.sum().item()
    print(f"  Wrong (sum): {wrong_sum:.6f}")
    
    # æ–¹æ³•2ï¼šæ­£ç¡®çš„æ–¹æ³•ï¼ˆä½¿ç”¨loss_maskï¼‰
    from beyondagent.module.advantage_assignment.parallel_semantic_assignment import _get_overall_advantage
    correct_value = _get_overall_advantage(sample_adv, effective_mask)
    print(f"  Correct (using proper mask): {correct_value:.6f}")
    
    # æ–¹æ³•3ï¼šä»…å–ç¬¬ä¸€ä¸ªæœ‰æ•ˆå€¼
    if effective_mask is not None:
        valid_advantages = sample_adv[effective_mask.bool()]
        if len(valid_advantages) > 0:
            first_valid = valid_advantages[0].item()
            print(f"  First valid token: {first_valid:.6f}")
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬éƒ½éµå¾ªGRPOæ¨¡å¼
    print(f"\nBatch-level GRPO pattern check:")
    grpo_compliant_samples = 0
    for i in range(min(5, advantages.shape[0])):  # åªæ£€æŸ¥å‰5ä¸ªæ ·æœ¬
        sample = advantages[i]
        
        # ä½¿ç”¨æ­£ç¡®çš„mask
        if loss_mask is not None:
            valid_vals = sample[loss_mask[i].bool()]
        elif response_mask is not None:
            valid_vals = sample[response_mask[i].bool()]
        else:
            valid_vals = sample[torch.abs(sample) > 1e-8]
        
        if len(valid_vals) > 1:
            all_same = torch.allclose(valid_vals, valid_vals[0], atol=1e-6)
            print(f"  Sample {i}: {len(valid_vals)} valid tokens, all same: {all_same}")
            if all_same:
                grpo_compliant_samples += 1
        else:
            print(f"  Sample {i}: {len(valid_vals)} valid tokens")
            grpo_compliant_samples += 1
    
    print(f"  GRPO compliant: {grpo_compliant_samples}/{min(5, advantages.shape[0])}")
    
    # å¦‚æœæœ‰step_idsï¼Œæ£€æŸ¥stepçº§åˆ«çš„advantageåˆ†å¸ƒ
    if "step_ids" in batch.batch:
        step_ids = batch.batch["step_ids"][sample_idx]
        print(f"\nStep-level advantage analysis for sample {sample_idx}:")
        unique_step_ids = torch.unique(step_ids[step_ids >= 0])
        for step_id in unique_step_ids:
            step_mask = (step_ids == step_id)
            step_advantages = sample_adv[step_mask]
            if len(step_advantages) > 0:
                print(f"  Step {step_id.item()}: {len(step_advantages)} tokens, "
                      f"value: {step_advantages[0].item():.6f}, "
                      f"all same: {torch.allclose(step_advantages, step_advantages[0]) if len(step_advantages) > 1 else True}")
    
    # æ–°å¢ï¼šloss_maskä¸step_idsçš„å¯¹åº”å…³ç³»æ£€æŸ¥
    if "step_ids" in batch.batch and loss_mask is not None:
        print(f"\nStep-ids vs Loss-mask correspondence check:")
        step_ids = batch.batch["step_ids"][sample_idx]
        sample_loss_mask = loss_mask[sample_idx]
        
        for step_id in torch.unique(step_ids[step_ids >= 0]):
            step_mask = (step_ids == step_id)
            step_loss_mask_values = sample_loss_mask[step_mask]
            
            if len(step_loss_mask_values) > 0:
                all_trainable = step_loss_mask_values.all().item()
                any_trainable = step_loss_mask_values.any().item()
                print(f"  Step {step_id.item()}: {step_mask.sum().item()} tokens, "
                      f"all trainable: {all_trainable}, any trainable: {any_trainable}")
    
    print("="*50)

def validate_grpo_advantage_structure(advantages, response_mask=None, loss_mask=None, tolerance=1e-6):
    """
    éªŒè¯advantageç»“æ„æ˜¯å¦ç¬¦åˆGRPOè¦æ±‚
    
    Args:
        advantages: advantage tensor, shape (bs, resp_len)
        response_mask: response mask (deprecated, use loss_mask instead)
        loss_mask: loss mask for response part, shape (bs, resp_len)
        tolerance: tolerance for floating point comparison
    
    Returns:
        bool: æ˜¯å¦ç¬¦åˆGRPOç»“æ„
        str: è¯¦ç»†è¯´æ˜
    """
    issues = []
    
    batch_size, seq_len = advantages.shape
    
    # ä¼˜å…ˆä½¿ç”¨loss_mask
    effective_mask = loss_mask if loss_mask is not None else response_mask
    
    for i in range(batch_size):
        sample_adv = advantages[i]
        
        if effective_mask is not None:
            valid_advantages = sample_adv[effective_mask[i].bool()]
        else:
            valid_advantages = sample_adv[torch.abs(sample_adv) > 1e-8]
        
        if len(valid_advantages) == 0:
            continue  # è·³è¿‡å…¨é›¶çš„æ ·æœ¬
        
        # æ£€æŸ¥æ‰€æœ‰æœ‰æ•ˆtokenæ˜¯å¦æœ‰ç›¸åŒçš„advantage
        if len(valid_advantages) > 1:
            if not torch.allclose(valid_advantages, valid_advantages[0], atol=tolerance):
                unique_vals = torch.unique(valid_advantages)
                issues.append(f"Sample {i}: Found {len(unique_vals)} different advantage values: {unique_vals[:5].tolist()}")
    
    is_valid = len(issues) == 0
    mask_type = "loss_mask" if loss_mask is not None else ("response_mask" if response_mask is not None else "non-zero values")
    
    if is_valid:
        return True, f"âœ… All samples follow GRPO advantage structure (same value for all valid tokens using {mask_type})"
    else:
        return False, f"âŒ GRPO structure violations (using {mask_type}):\n" + "\n".join(issues[:5])

def add_debug_to_trainer_fit():
    """
    åœ¨fitå‡½æ•°ä¸­æ·»åŠ è°ƒè¯•ä»£ç çš„å»ºè®®ä½ç½®
    """
    debug_code = '''
    # åœ¨compute_advantageä¹‹åï¼Œsemanticå¤„ç†ä¹‹å‰æ·»åŠ ï¼š
    print("ğŸ” [DEBUG] Checking advantage structure before semantic processing...")
    debug_advantage_structure(batch, self.tokenizer)  # è‡ªåŠ¨é€‰æ‹©éé›¶æ ·æœ¬
    
    # æ£€æŸ¥å‰å‡ ä¸ªæ ·æœ¬çš„advantageå€¼èŒƒå›´
    advs = batch.batch["advantages"]
    print(f"ğŸ” [DEBUG] Advantage stats - Shape: {advs.shape}")
    print(f"ğŸ” [DEBUG] Advantage range: [{advs.min().item():.6f}, {advs.max().item():.6f}]")
    print(f"ğŸ” [DEBUG] Advantage mean: {advs.mean().item():.6f}")
    
    # ä½¿ç”¨æ­£ç¡®çš„loss_maskè¿›è¡ŒéªŒè¯
    response_length = advs.shape[1]
    loss_mask_response = batch.batch["loss_mask"][:, -response_length:]
    
    is_valid, message = validate_grpo_advantage_structure(
        advs, 
        loss_mask=loss_mask_response
    )
    print(f"ğŸ” [GRPO Validation] {message}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰advantage=0çš„æ ·æœ¬ï¼ˆä½¿ç”¨æ­£ç¡®çš„maskï¼‰
    from beyondagent.module.advantage_assignment.parallel_semantic_assignment import _get_overall_advantage
    zero_count = 0
    for i in range(len(batch)):
        adv_val = _get_overall_advantage(advs[i], loss_mask_response[i])
        if abs(adv_val) < 1e-8:
            zero_count += 1
    
    print(f"ğŸ” [Zero Advantage] {zero_count}/{len(batch)} samples have advantageâ‰ˆ0 (using loss_mask)")
    '''
    return debug_code